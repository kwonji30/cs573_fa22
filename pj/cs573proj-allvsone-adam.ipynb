{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47294315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c608a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMNIST(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, transform=None):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, we use ToTensor(), so we define the numpy array like (H, W, C)\n",
    "        image = self.data.iloc[index, 1:].values.astype(np.uint8).reshape((28, 28, 1))\n",
    "        label = self.data.iloc[index, 0]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8bc99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aade534",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './out0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data0 \u001b[38;5;241m=\u001b[39m \u001b[43mTrainMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./out0.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_data1 \u001b[38;5;241m=\u001b[39m TrainMNIST(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./out1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[1;32m      3\u001b[0m train_data2 \u001b[38;5;241m=\u001b[39m TrainMNIST(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./out2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mTrainMNIST.__init__\u001b[0;34m(self, file_path, transform)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './out0.csv'"
     ]
    }
   ],
   "source": [
    "train_data0 = TrainMNIST(\"./out0.csv\", transform=transforms.ToTensor())\n",
    "train_data1 = TrainMNIST(\"./out1.csv\", transform=transforms.ToTensor())\n",
    "train_data2 = TrainMNIST(\"./out2.csv\", transform=transforms.ToTensor())\n",
    "train_data3 = TrainMNIST(\"./out3.csv\", transform=transforms.ToTensor())\n",
    "train_data4 = TrainMNIST(\"./out4.csv\", transform=transforms.ToTensor())\n",
    "train_data5 = TrainMNIST(\"./out5.csv\", transform=transforms.ToTensor())\n",
    "train_data6 = TrainMNIST(\"./out6.csv\", transform=transforms.ToTensor())\n",
    "train_data7 = TrainMNIST(\"./out7.csv\", transform=transforms.ToTensor())\n",
    "train_data8 = TrainMNIST(\"./out8.csv\", transform=transforms.ToTensor())\n",
    "train_data9 = TrainMNIST(\"./out9.csv\", transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee86c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv(\"./mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b986bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain training indices that will be used for validation\n",
    "\n",
    "num_train = len(train_data0)\n",
    "\n",
    "indices = list(range(num_train))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader0 = torch.utils.data.DataLoader(train_data0, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader1 = torch.utils.data.DataLoader(train_data1, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader2 = torch.utils.data.DataLoader(train_data2, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader3 = torch.utils.data.DataLoader(train_data3, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader4 = torch.utils.data.DataLoader(train_data4, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader5 = torch.utils.data.DataLoader(train_data5, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader6 = torch.utils.data.DataLoader(train_data6, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader7 = torch.utils.data.DataLoader(train_data7, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader8 = torch.utils.data.DataLoader(train_data8, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)\n",
    "train_loader9 = torch.utils.data.DataLoader(train_data9, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2ec2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# define NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 512\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(512,1)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        # self.droput = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1,28*28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        #x = self.droput(x)\n",
    "         # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        #x = self.droput(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model0 = Net()\n",
    "model1 = Net()\n",
    "model2 = Net()\n",
    "model3 = Net()\n",
    "model4 = Net()\n",
    "model5 = Net()\n",
    "model6 = Net()\n",
    "model7 = Net()\n",
    "model8 = Net()\n",
    "model9 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5ca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer0 = torch.optim.Adam(model0.parameters(),lr = 0.001)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(),lr = 0.001)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(),lr = 0.001)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(),lr = 0.001)\n",
    "optimizer4 = torch.optim.Adam(model4.parameters(),lr = 0.001)\n",
    "optimizer5 = torch.optim.Adam(model5.parameters(),lr = 0.001)\n",
    "optimizer6 = torch.optim.Adam(model6.parameters(),lr = 0.001)\n",
    "optimizer7 = torch.optim.Adam(model7.parameters(),lr = 0.001)\n",
    "optimizer8 = torch.optim.Adam(model8.parameters(),lr = 0.001)\n",
    "optimizer9 = torch.optim.Adam(model9.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e721ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317badec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  1314.7392196288947\n",
      "train_loss:  610.0603990245032\n",
      "train_loss:  430.3833579944823\n",
      "train_loss:  316.361058386926\n",
      "train_loss:  250.99071701696792\n",
      "train_loss:  211.62202945141857\n",
      "train_loss:  246.57617576187354\n",
      "train_loss:  193.1184278549968\n",
      "train_loss:  185.8167427530442\n",
      "train_loss:  134.7316269218431\n",
      "train_loss:  105.48945804632649\n",
      "train_loss:  110.36889811131776\n",
      "train_loss:  123.21038194086665\n",
      "train_loss:  91.10601509707061\n",
      "train_loss:  82.30077194359922\n",
      "train_loss:  96.30772745049092\n",
      "train_loss:  95.9614367080674\n",
      "train_loss:  4.6233051157789085\n",
      "train_loss:  7.709361126802907\n",
      "train_loss:  191.5532350636593\n",
      "train_loss:  48.431722589753726\n",
      "train_loss:  87.12018003138957\n",
      "train_loss:  49.17152283056199\n",
      "train_loss:  48.76957406116925\n",
      "train_loss:  32.37987608095246\n",
      "train_loss:  62.37546276191165\n",
      "train_loss:  63.515461876134886\n",
      "train_loss:  67.81451567912399\n",
      "train_loss:  60.26312602559806\n",
      "train_loss:  0.2110456260214022\n",
      "train_loss:  0.002027662737962288\n",
      "train_loss:  0.0002082536964387316\n",
      "train_loss:  4.279604302936946e-05\n",
      "train_loss:  1.9192675875245868e-05\n",
      "train_loss:  8.344645898716863e-06\n",
      "train_loss:  2.741813105444635e-06\n",
      "train_loss:  1.3113020536081876e-06\n",
      "train_loss:  5.960464122267695e-07\n",
      "train_loss:  3.576278473360617e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:17.18m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    model0.train() # prep model for training\n",
    "    for data,label in train_loader0:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer0.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model0(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer0.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)\n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model0.state_dict(),\"1vA0.pt\")\n",
    "        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30198ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  1324.0814468786743\n",
      "train_loss:  618.5068852758673\n",
      "train_loss:  432.96082455137895\n",
      "train_loss:  276.88444626471426\n",
      "train_loss:  249.20798008566246\n",
      "train_loss:  199.80496474635888\n",
      "train_loss:  233.94871574398152\n",
      "train_loss:  166.23898156640814\n",
      "train_loss:  147.93640385481308\n",
      "train_loss:  105.87853439520785\n",
      "train_loss:  136.68003464511787\n",
      "train_loss:  89.30785263553801\n",
      "train_loss:  117.91307838687715\n",
      "train_loss:  75.59077567457086\n",
      "train_loss:  72.64236838493022\n",
      "train_loss:  46.03348489868135\n",
      "train_loss:  124.50901005821692\n",
      "train_loss:  89.61577445868886\n",
      "train_loss:  20.377671355777185\n",
      "train_loss:  61.54369122160929\n",
      "train_loss:  35.33921032872813\n",
      "train_loss:  107.78307800639917\n",
      "train_loss:  35.60230588698455\n",
      "train_loss:  57.90849529602895\n",
      "train_loss:  4.470573880341\n",
      "train_loss:  20.656410433863037\n",
      "train_loss:  98.619417452984\n",
      "train_loss:  33.87575136286954\n",
      "train_loss:  3.3041252553773504\n",
      "train_loss:  77.96991954430788\n",
      "train_loss:  96.36282316063519\n",
      "train_loss:  1.4401126853739221\n",
      "train_loss:  58.7511725008773\n",
      "train_loss:  104.61191616955188\n",
      "train_loss:  0.06202890449156229\n",
      "train_loss:  0.016452009562542003\n",
      "train_loss:  0.006347602886460635\n",
      "train_loss:  0.0015983895015381222\n",
      "train_loss:  0.00033092278888702253\n",
      "train_loss:  9.751301515592559e-05\n",
      "train_loss:  3.421304265671665e-05\n",
      "train_loss:  1.0848042748534681e-05\n",
      "train_loss:  4.410742935334611e-06\n",
      "train_loss:  1.5497206540260322e-06\n",
      "train_loss:  4.7683712978141557e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:20.18m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model1.train() # prep model for training\n",
    "    for data,label in train_loader1:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer1.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model1(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer1.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)\n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model1.state_dict(),\"1vA1.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd2895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  2214.565202165031\n",
      "train_loss:  1041.2735111653296\n",
      "train_loss:  763.8278479066241\n",
      "train_loss:  567.1449773734082\n",
      "train_loss:  461.7640383348025\n",
      "train_loss:  375.4903426338373\n",
      "train_loss:  286.8904433065578\n",
      "train_loss:  276.1371469380859\n",
      "train_loss:  211.65379761684375\n",
      "train_loss:  174.96892964958943\n",
      "train_loss:  185.99854462434848\n",
      "train_loss:  166.50956660105138\n",
      "train_loss:  135.02556372712633\n",
      "train_loss:  97.31168298910582\n",
      "train_loss:  140.61975940653292\n",
      "train_loss:  117.94085073535035\n",
      "train_loss:  147.55969898655343\n",
      "train_loss:  101.08833708715623\n",
      "train_loss:  155.794673762466\n",
      "train_loss:  78.64713645942246\n",
      "train_loss:  84.34357114439592\n",
      "train_loss:  145.48841222213323\n",
      "train_loss:  50.91842619001113\n",
      "train_loss:  123.03991341651306\n",
      "train_loss:  86.52536339813436\n",
      "train_loss:  52.49970977452982\n",
      "train_loss:  37.341656514111946\n",
      "train_loss:  80.74044173718889\n",
      "train_loss:  136.32165083946575\n",
      "train_loss:  148.6356912776132\n",
      "train_loss:  63.72642457490237\n",
      "train_loss:  78.29215124325512\n",
      "train_loss:  38.540611639058966\n",
      "train_loss:  73.66584199740402\n",
      "train_loss:  69.70305250188267\n",
      "train_loss:  10.927171126205355\n",
      "train_loss:  136.5220100085317\n",
      "train_loss:  52.07158342159698\n",
      "train_loss:  73.4195687978436\n",
      "train_loss:  73.33359697139451\n",
      "train_loss:  28.437363891912266\n",
      "train_loss:  128.45916397321218\n",
      "train_loss:  36.269391788573415\n",
      "train_loss:  28.07792513856988\n",
      "train_loss:  96.0693583003567\n",
      "train_loss:  40.35872655482023\n",
      "train_loss:  29.145986925366643\n",
      "train_loss:  2.6101026851630316\n",
      "train_loss:  0.013068621552019266\n",
      "train_loss:  0.001759901505806738\n",
      "train_loss:  0.0005494272135209144\n",
      "train_loss:  0.00018632318143474436\n",
      "train_loss:  5.924693036618578e-05\n",
      "train_loss:  2.026556995105011e-05\n",
      "train_loss:  6.31809159656882e-06\n",
      "train_loss:  2.2649763131710188e-06\n",
      "train_loss:  4.7683712978141557e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  2.384185471271394e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:24.74m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model2.train() # prep model for training\n",
    "    for data,label in train_loader2:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer2.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model2(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer2.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)\n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model2.state_dict(),\"1vA2.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d500a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  2773.3420979382936\n",
      "train_loss:  1286.5525435027416\n",
      "train_loss:  899.9850413524331\n",
      "train_loss:  686.0126300973786\n",
      "train_loss:  519.983785110267\n",
      "train_loss:  425.6022817560597\n",
      "train_loss:  328.9258818704076\n",
      "train_loss:  357.2159484785228\n",
      "train_loss:  254.1295428811111\n",
      "train_loss:  286.49140966140754\n",
      "train_loss:  231.0375194748326\n",
      "train_loss:  195.04065265803635\n",
      "train_loss:  191.93634735402358\n",
      "train_loss:  211.65892611974056\n",
      "train_loss:  171.35809154745658\n",
      "train_loss:  139.7642382618015\n",
      "train_loss:  124.84896590260895\n",
      "train_loss:  142.38407982793214\n",
      "train_loss:  128.1912815785575\n",
      "train_loss:  161.88428553013568\n",
      "train_loss:  196.99769408006807\n",
      "train_loss:  92.74064181672642\n",
      "train_loss:  128.6556156130524\n",
      "train_loss:  98.08603167884613\n",
      "train_loss:  120.04937260835229\n",
      "train_loss:  143.57050073844235\n",
      "train_loss:  115.59011751409741\n",
      "train_loss:  44.46819749987916\n",
      "train_loss:  125.12578861667441\n",
      "train_loss:  167.60058518446286\n",
      "train_loss:  150.0188234833906\n",
      "train_loss:  38.77393502725399\n",
      "train_loss:  88.19991016885365\n",
      "train_loss:  155.63855153429492\n",
      "train_loss:  91.23720365510994\n",
      "train_loss:  92.2384319395884\n",
      "train_loss:  175.4949451225055\n",
      "train_loss:  47.74741229174998\n",
      "train_loss:  105.83432680093709\n",
      "train_loss:  53.5334543233686\n",
      "train_loss:  127.08469773658817\n",
      "train_loss:  51.76035670340627\n",
      "train_loss:  39.40660451898078\n",
      "train_loss:  84.27671534288683\n",
      "train_loss:  170.99675267058032\n",
      "train_loss:  106.90156503353325\n",
      "train_loss:  112.9781211004599\n",
      "train_loss:  116.92531881998102\n",
      "train_loss:  78.98388606195536\n",
      "train_loss:  66.04617979320471\n",
      "train_loss:  42.25560170943033\n",
      "train_loss:  65.23072608879349\n",
      "train_loss:  47.28563822233829\n",
      "train_loss:  135.7828352976231\n",
      "train_loss:  58.17925805746068\n",
      "train_loss:  14.799966541296214\n",
      "train_loss:  106.97804524736384\n",
      "train_loss:  0.7218386428916546\n",
      "train_loss:  0.019622687741449596\n",
      "train_loss:  0.005522762907190781\n",
      "train_loss:  0.002102544318951516\n",
      "train_loss:  0.0005440601923645261\n",
      "train_loss:  0.0002225607588712819\n",
      "train_loss:  5.030622860502376e-05\n",
      "train_loss:  1.2993808464756285e-05\n",
      "train_loss:  4.291533848288509e-06\n",
      "train_loss:  1.1920928066899705e-06\n",
      "train_loss:  3.576278473360617e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:30.64m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model3.train() # prep model for training\n",
    "    for data,label in train_loader3:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer3.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model3(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer3.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)   \n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model3.state_dict(),\"1vA3.pt\")    \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c297bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  2008.4292898032845\n",
      "train_loss:  878.3777637196215\n",
      "train_loss:  632.2027687528937\n",
      "train_loss:  502.546023355595\n",
      "train_loss:  363.2351045067647\n",
      "train_loss:  354.9513756382304\n",
      "train_loss:  284.79236005506914\n",
      "train_loss:  290.05262752049714\n",
      "train_loss:  217.8620191996819\n",
      "train_loss:  160.44941367786956\n",
      "train_loss:  168.17355896391757\n",
      "train_loss:  180.58825976646676\n",
      "train_loss:  116.35070989917975\n",
      "train_loss:  83.49961657082446\n",
      "train_loss:  157.43037809682656\n",
      "train_loss:  171.21341749172055\n",
      "train_loss:  124.219550277802\n",
      "train_loss:  107.51467745056917\n",
      "train_loss:  145.01169321063446\n",
      "train_loss:  90.1944108218289\n",
      "train_loss:  85.7808303727102\n",
      "train_loss:  110.3658047424041\n",
      "train_loss:  129.1790398024699\n",
      "train_loss:  48.24255741921102\n",
      "train_loss:  49.70353438315669\n",
      "train_loss:  102.5249006564566\n",
      "train_loss:  49.56824770340555\n",
      "train_loss:  59.06711677241337\n",
      "train_loss:  103.62651882834322\n",
      "train_loss:  176.06913705499477\n",
      "train_loss:  69.4920701230352\n",
      "train_loss:  43.100901704341105\n",
      "train_loss:  73.11055636451596\n",
      "train_loss:  105.59235691206916\n",
      "train_loss:  50.07756286799611\n",
      "train_loss:  56.59717860326392\n",
      "train_loss:  84.86491361319402\n",
      "train_loss:  39.55129539828472\n",
      "train_loss:  61.83322883877324\n",
      "train_loss:  0.5685094882828601\n",
      "train_loss:  75.83497840557723\n",
      "train_loss:  123.82796310976636\n",
      "train_loss:  36.74538720839214\n",
      "train_loss:  83.7949208127389\n",
      "train_loss:  0.1848304761235653\n",
      "train_loss:  0.023854974500352455\n",
      "train_loss:  0.0021144620305157957\n",
      "train_loss:  0.00032245781289574893\n",
      "train_loss:  0.0001099105255875088\n",
      "train_loss:  3.635879094687766e-05\n",
      "train_loss:  1.382827136353626e-05\n",
      "train_loss:  5.960463607124211e-06\n",
      "train_loss:  2.145767030725665e-06\n",
      "train_loss:  7.152556946721234e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:27.72m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model4.train() # prep model for training\n",
    "    for data,label in train_loader4:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer4.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model4(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer4.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)    \n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model4.state_dict(),\"1vA4.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2669205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNTIME:29.54m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model5.train() # prep model for training\n",
    "    for data,label in train_loader5:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer5.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model5(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer5.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model5.state_dict(),\"1vA5.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814e3028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  1515.6080651654975\n",
      "train_loss:  681.7469765241574\n",
      "train_loss:  471.2277821481304\n",
      "train_loss:  364.2931436490879\n",
      "train_loss:  302.34252340080457\n",
      "train_loss:  265.92443176296234\n",
      "train_loss:  221.04416528195668\n",
      "train_loss:  222.17781515814275\n",
      "train_loss:  143.2388746300736\n",
      "train_loss:  134.8261598868095\n",
      "train_loss:  177.7373786235088\n",
      "train_loss:  130.41679876102543\n",
      "train_loss:  128.8532160818957\n",
      "train_loss:  67.2742446983809\n",
      "train_loss:  187.2430817025596\n",
      "train_loss:  96.05383899372765\n",
      "train_loss:  121.81998770972328\n",
      "train_loss:  43.348347533833895\n",
      "train_loss:  152.16781086308663\n",
      "train_loss:  91.89149874225339\n",
      "train_loss:  91.3480412809418\n",
      "train_loss:  38.98400982084237\n",
      "train_loss:  46.981889890819836\n",
      "train_loss:  108.09521965270028\n",
      "train_loss:  79.84005803688041\n",
      "train_loss:  29.874191692762224\n",
      "train_loss:  50.16583170374322\n",
      "train_loss:  134.6050261401988\n",
      "train_loss:  25.36549236963541\n",
      "train_loss:  0.16659294294731453\n",
      "train_loss:  0.011344946861662208\n",
      "train_loss:  0.0016527655381359807\n",
      "train_loss:  0.000495786789276309\n",
      "train_loss:  0.00015747497259965826\n",
      "train_loss:  4.971022733002428e-05\n",
      "train_loss:  1.6450876358931055e-05\n",
      "train_loss:  6.198882296359898e-06\n",
      "train_loss:  1.668929936471386e-06\n",
      "train_loss:  3.576278473360617e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:18.75m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model6.train() # prep model for training\n",
    "    for data,label in train_loader6:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer6.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model6(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer6.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)    \n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model6.state_dict(),\"1vA6.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d90d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  2045.9472260611165\n",
      "train_loss:  1140.1946531244266\n",
      "train_loss:  763.9229139498855\n",
      "train_loss:  583.4575274081976\n",
      "train_loss:  473.0209475095982\n",
      "train_loss:  345.63680243034423\n",
      "train_loss:  240.39019542412075\n",
      "train_loss:  265.25442368502115\n",
      "train_loss:  260.3951600260342\n",
      "train_loss:  194.2810157170783\n",
      "train_loss:  187.92129939907397\n",
      "train_loss:  196.72461862685515\n",
      "train_loss:  85.2083297365173\n",
      "train_loss:  143.68026750016196\n",
      "train_loss:  169.93631357567187\n",
      "train_loss:  157.38824072106488\n",
      "train_loss:  78.9568637870023\n",
      "train_loss:  87.7604801897294\n",
      "train_loss:  95.77387693034262\n",
      "train_loss:  94.58875622983587\n",
      "train_loss:  116.50925763633956\n",
      "train_loss:  114.50558172769942\n",
      "train_loss:  63.05956081203133\n",
      "train_loss:  89.57804597292599\n",
      "train_loss:  2.8216156887591914\n",
      "train_loss:  96.23196705796572\n",
      "train_loss:  176.1943646727157\n",
      "train_loss:  76.91794196382938\n",
      "train_loss:  54.51476283195956\n",
      "train_loss:  67.4749103687501\n",
      "train_loss:  73.41587957236953\n",
      "train_loss:  27.59021113995562\n",
      "train_loss:  149.72833099197808\n",
      "train_loss:  36.28931349629281\n",
      "train_loss:  0.5990001758540942\n",
      "train_loss:  103.00090821582273\n",
      "train_loss:  71.98063547874116\n",
      "train_loss:  134.7854727614038\n",
      "train_loss:  108.9604171725525\n",
      "train_loss:  32.75533428856323\n",
      "train_loss:  67.75866656315478\n",
      "train_loss:  0.12359254973688394\n",
      "train_loss:  0.013765459703503069\n",
      "train_loss:  0.0023368597100414945\n",
      "train_loss:  0.0006316806544326425\n",
      "train_loss:  0.00015330268658075852\n",
      "train_loss:  4.82797193868123e-05\n",
      "train_loss:  1.6212459144071545e-05\n",
      "train_loss:  6.31809159656882e-06\n",
      "train_loss:  2.2649763486981556e-06\n",
      "train_loss:  5.960464122267695e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:23.46m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model7.train() # prep model for training\n",
    "    for data,label in train_loader7:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer7.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model7(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer7.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)    \n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model7.state_dict(),\"1vA7.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d4f471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  2856.367921546407\n",
      "train_loss:  1284.107532825176\n",
      "train_loss:  984.042816597908\n",
      "train_loss:  754.0874921505252\n",
      "train_loss:  642.4661081995055\n",
      "train_loss:  521.1657772207349\n",
      "train_loss:  435.2637877969286\n",
      "train_loss:  360.898500075672\n",
      "train_loss:  342.4198613699077\n",
      "train_loss:  301.71634746240534\n",
      "train_loss:  322.67883377415137\n",
      "train_loss:  286.01069675124967\n",
      "train_loss:  258.5578033704085\n",
      "train_loss:  204.15176397175603\n",
      "train_loss:  156.85776829551008\n",
      "train_loss:  175.95371576720234\n",
      "train_loss:  181.20546639893504\n",
      "train_loss:  158.70393841109734\n",
      "train_loss:  142.77380837889052\n",
      "train_loss:  156.04330896001815\n",
      "train_loss:  155.19609142877624\n",
      "train_loss:  71.10412086229803\n",
      "train_loss:  154.62796985898473\n",
      "train_loss:  88.11463379045765\n",
      "train_loss:  151.83617291736283\n",
      "train_loss:  86.44220893816313\n",
      "train_loss:  85.65159146489597\n",
      "train_loss:  90.51255054809388\n",
      "train_loss:  130.2062792561705\n",
      "train_loss:  65.39517926275587\n",
      "train_loss:  81.36484310245216\n",
      "train_loss:  65.99138235024924\n",
      "train_loss:  86.22752637912879\n",
      "train_loss:  95.10179316221382\n",
      "train_loss:  84.83837163405413\n",
      "train_loss:  68.88677550311174\n",
      "train_loss:  81.72810692034724\n",
      "train_loss:  130.37804942404023\n",
      "train_loss:  94.64907574688355\n",
      "train_loss:  56.4747303800375\n",
      "train_loss:  48.96223968632006\n",
      "train_loss:  92.09508565480793\n",
      "train_loss:  44.67620472105763\n",
      "train_loss:  57.0135934594701\n",
      "train_loss:  87.16181760105115\n",
      "train_loss:  51.69140066804091\n",
      "train_loss:  30.519720593677704\n",
      "train_loss:  93.51522481521485\n",
      "train_loss:  66.07374152247499\n",
      "train_loss:  2.492865971720377\n",
      "train_loss:  95.91757831833445\n",
      "train_loss:  88.48179546154444\n",
      "train_loss:  38.39436669638289\n",
      "train_loss:  77.47822170866678\n",
      "train_loss:  61.80771936201638\n",
      "train_loss:  90.87398434739191\n",
      "train_loss:  6.487980937815987\n",
      "train_loss:  0.11734114668534801\n",
      "train_loss:  0.02307767043134845\n",
      "train_loss:  0.004722291088956609\n",
      "train_loss:  0.0013233258360756395\n",
      "train_loss:  0.00047349558975895434\n",
      "train_loss:  0.00014996482587292803\n",
      "train_loss:  4.100796690664765e-05\n",
      "train_loss:  1.3470646837987488e-05\n",
      "train_loss:  3.695487595933855e-06\n",
      "train_loss:  9.536742417992627e-07\n",
      "train_loss:  3.576278473360617e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:38.65m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model8.train() # prep model for training\n",
    "    for data,label in train_loader8:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer8.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model8(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer8.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)   \n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model8.state_dict(),\"1vA8.pt\")     \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9ef5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  2899.3432612545075\n",
      "train_loss:  1406.832280722083\n",
      "train_loss:  1047.4885530558004\n",
      "train_loss:  787.2264593605041\n",
      "train_loss:  682.4731100198076\n",
      "train_loss:  544.3151176310641\n",
      "train_loss:  388.427544208934\n",
      "train_loss:  397.0303505838763\n",
      "train_loss:  302.33096697554413\n",
      "train_loss:  284.4231291227095\n",
      "train_loss:  257.3880902277075\n",
      "train_loss:  217.09021209651607\n",
      "train_loss:  218.48487431448106\n",
      "train_loss:  180.54216615488096\n",
      "train_loss:  185.62142455397844\n",
      "train_loss:  145.13959472558645\n",
      "train_loss:  158.2806650132885\n",
      "train_loss:  166.62450722713533\n",
      "train_loss:  135.00048384879716\n",
      "train_loss:  122.63598789539589\n",
      "train_loss:  117.41127871282379\n",
      "train_loss:  95.43857672082372\n",
      "train_loss:  126.5289026946835\n",
      "train_loss:  115.19777845100481\n",
      "train_loss:  117.22447041140005\n",
      "train_loss:  98.88039820905642\n",
      "train_loss:  131.47203053280617\n",
      "train_loss:  37.03271440053802\n",
      "train_loss:  151.9444411677788\n",
      "train_loss:  35.97456648506494\n",
      "train_loss:  52.53795533337074\n",
      "train_loss:  128.86612145757528\n",
      "train_loss:  108.81619641968655\n",
      "train_loss:  37.23726190939242\n",
      "train_loss:  141.94144019819765\n",
      "train_loss:  56.07217641489551\n",
      "train_loss:  45.662583477209196\n",
      "train_loss:  71.11377080167578\n",
      "train_loss:  67.02615576346977\n",
      "train_loss:  34.607646117555305\n",
      "train_loss:  65.54819460718758\n",
      "train_loss:  112.0348697443365\n",
      "train_loss:  102.046327053227\n",
      "train_loss:  119.378696458017\n",
      "train_loss:  124.21282473136273\n",
      "train_loss:  62.90538626577641\n",
      "train_loss:  87.91383764725927\n",
      "train_loss:  154.66618218108616\n",
      "train_loss:  45.50784059819379\n",
      "train_loss:  88.69899256482526\n",
      "train_loss:  60.87381644314449\n",
      "train_loss:  32.42726825724243\n",
      "train_loss:  142.0203178939807\n",
      "train_loss:  48.969643981970265\n",
      "train_loss:  47.94248568915902\n",
      "train_loss:  37.0002203233874\n",
      "train_loss:  78.04917304959918\n",
      "train_loss:  14.469868617206778\n",
      "train_loss:  115.7009409141213\n",
      "train_loss:  31.53369853152414\n",
      "train_loss:  57.12275423525298\n",
      "train_loss:  85.00013426187255\n",
      "train_loss:  39.16165453724094\n",
      "train_loss:  109.58551887192576\n",
      "train_loss:  12.87786681030898\n",
      "train_loss:  51.83720201651057\n",
      "train_loss:  2.112110409742538\n",
      "train_loss:  0.19901530793228872\n",
      "train_loss:  0.011948347866059805\n",
      "train_loss:  0.001089416687864997\n",
      "train_loss:  0.00028490899333988295\n",
      "train_loss:  8.547295859528958e-05\n",
      "train_loss:  3.218649275993357e-05\n",
      "train_loss:  1.27553918538581e-05\n",
      "train_loss:  5.0067896850691795e-06\n",
      "train_loss:  1.78813921891674e-06\n",
      "train_loss:  4.7683712978141557e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  1.1920928244535389e-07\n",
      "train_loss:  2.3841856489070778e-07\n",
      "train_loss:  0.0\n",
      "RUNTIME:38.07m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    \n",
    "    model9.train() # prep model for training\n",
    "    for data,label in train_loader9:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer9.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model9(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output.view(20),label.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer9.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    print(\"train_loss: \",train_loss)    \n",
    "    if train_loss < 0.00000001:\n",
    "        break\n",
    "\n",
    "torch.save(model9.state_dict(),\"1vA9.pt\")        \n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87be50bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "model0.load_state_dict(torch.load('./ova/1vA0.pt'))\n",
    "model1.load_state_dict(torch.load('./ova/1vA1.pt'))\n",
    "model2.load_state_dict(torch.load('./ova/1vA2.pt'))\n",
    "model3.load_state_dict(torch.load('./ova/1vA3.pt'))\n",
    "model4.load_state_dict(torch.load('./ova/1vA4.pt'))\n",
    "model5.load_state_dict(torch.load('./ova/1vA5.pt'))\n",
    "model6.load_state_dict(torch.load('./ova/1vA6.pt'))\n",
    "model7.load_state_dict(torch.load('./ova/1vA7.pt'))\n",
    "model8.load_state_dict(torch.load('./ova/1vA8.pt'))\n",
    "model9.load_state_dict(torch.load('./ova/1vA9.pt'))\n",
    "\n",
    "models.append(model0)\n",
    "models.append(model1)\n",
    "models.append(model2)\n",
    "models.append(model3)\n",
    "models.append(model4)\n",
    "models.append(model5)\n",
    "models.append(model6)\n",
    "models.append(model7)\n",
    "models.append(model8)\n",
    "models.append(model9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "244b370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "fc1weight = []\n",
    "fc1bias = []\n",
    "fc2weight = []\n",
    "fc2bias = []\n",
    "fc3weight = []\n",
    "fc3bias = []\n",
    "\n",
    "params.append(fc1weight)\n",
    "params.append(fc1bias)\n",
    "params.append(fc2weight)\n",
    "params.append(fc2bias)\n",
    "params.append(fc3weight)\n",
    "params.append(fc3bias)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1422126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): \n",
    "    fc1weight.append(models[i].state_dict()['fc1.weight'])\n",
    "    fc1bias.append(models[i].state_dict()['fc1.bias'])\n",
    "    fc2weight.append(models[i].state_dict()['fc2.weight'])\n",
    "    fc2bias.append(models[i].state_dict()['fc2.bias'])\n",
    "    fc3weight.append(models[i].state_dict()['fc3.weight'])\n",
    "    fc3bias.append(models[i].state_dict()['fc3.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d7a63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for i in range(6):\n",
    "    means.append(sum(params[i])/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f9a9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars=[]\n",
    "for i in range(6):\n",
    "    sqrdiff =  [(x-means[i])**2 for x in params[i]]\n",
    "    vars.append(sum(sqrdiff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6eb63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07a50d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34793\n",
      "25\n",
      "8066\n",
      "130\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "notmul = []\n",
    "mult = []\n",
    "for i in range(6):\n",
    "    print(torch.numel(vars[i][vars[i]>thresh]))\n",
    "    mult.append(vars[i]>thresh)\n",
    "    notmul.append(vars[i] <=thresh)\n",
    "    mult[i] = mult[i].float()\n",
    "    notmul[i] = notmul[i].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "310a116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0., -0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., 0.,  ..., 0., -0., -0.],\n",
      "        [-0., 0., 0.,  ..., -0., -0., 0.],\n",
      "        ...,\n",
      "        [0., -0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., -0., -0., -0.],\n",
      "        [-0., 0., 0.,  ..., -0., -0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(means[0] * mult[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60cd5e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mtd\u001b[49m)\n\u001b[1;32m      3\u001b[0m x_test , y_test \u001b[38;5;241m=\u001b[39m test[:,\u001b[38;5;241m1\u001b[39m:], test[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "test = np.array(td)\n",
    "\n",
    "x_test , y_test = test[:,1:], test[:,0]\n",
    "y = torch.from_numpy(y_test)\n",
    "x = torch.from_numpy(x_test)\n",
    "print(len(y))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "728d0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2, 5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1, 7, 1, 8, 2, 0, 9, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5, 1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1, 0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0, 3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1, 5, 9, 8, 7, 2, 3, 0, 6, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 6, 8, 5, 7, 7, 9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4, 1, 5, 8, 2, 9, 2, 0, 4, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 7, 4, 3, 3, 0, 0, 3, 1, 9, 6, 5, 2, 5, 9, 7, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3, 9, 7, 8, 6, 3, 6, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 5, 6, 1, 8, 5, 1, 7, 9, 4, 6, 2, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 7, 3, 7, 6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0, 3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8, 4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 2, 0, 6, 4, 9, 3, 3, 3, 2, 3, 9, 1, 2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 7, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 9, 1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 2, 9, 4, 0, 6, 3, 9, 5, 2, 1, 3, 1, 3, 6, 5, 7, 4, 2, 2, 6, 3, 2, 6, 5, 4, 8, 9, 7, 1, 3, 0, 3, 8, 3, 1, 9, 3, 4, 4, 6, 4, 2, 1, 8, 2, 5, 4, 8, 3, 4, 0, 0, 2, 3, 2, 7, 7, 0, 8, 7, 4, 4, 7, 9, 6, 9, 0, 9, 8, 0, 4, 6, 0, 6, 3, 5, 4, 8, 3, 3, 9, 3, 3, 7, 7, 8, 0, 2, 7, 1, 7, 0, 6, 5, 4, 3, 8, 0, 9, 6, 3, 8, 0, 9, 9, 6, 8, 6, 8, 5, 7, 8, 6, 0, 2, 4, 0, 2, 2, 3, 1, 9, 7, 5, 1, 0, 8, 4, 6, 2, 6, 7, 9, 3, 2, 9, 8, 2, 2, 9, 2, 7, 3, 5, 9, 1, 8, 0, 2, 0, 5, 2, 1, 3, 7, 6, 7, 1, 2, 5, 8, 0, 3, 7, 3, 4, 0, 9, 1, 8, 6, 7, 7, 4, 3, 4, 9, 1, 9, 5, 1, 7, 3, 9, 7, 6, 9, 1, 3, 7, 8, 3, 3, 6, 7, 2, 4, 5, 8, 5, 1, 1, 4, 4, 3, 1, 0, 7, 7, 0, 7, 9, 4, 4, 8, 5, 5, 4, 0, 8, 2, 1, 6, 8, 4, 8, 0, 4, 0, 6, 1, 7, 3, 2, 6, 7, 2, 6, 9, 3, 1, 4, 6, 2, 5, 4, 2, 0, 6, 2, 1, 7, 3, 4, 1, 0, 5, 4, 3, 1, 1, 7, 4, 9, 9, 4, 8, 4, 0, 2, 4, 5, 1, 1, 6, 4, 7, 1, 9, 4, 2, 4, 1, 5, 5, 3, 8, 3, 1, 4, 5, 6, 8, 9, 4, 1, 5, 3, 8, 0, 3, 2, 5, 1, 2, 8, 3, 4, 4, 0, 8, 8, 3, 3, 1, 7, 3, 5, 9, 6, 3, 2, 6, 1, 3, 6, 0, 7, 2, 1, 7, 1, 4, 2, 4, 2, 1, 7, 9, 6, 1, 1, 2, 4, 8, 1, 7, 7, 4, 8, 0, 7, 3, 1, 3, 1, 0, 7, 7, 0, 3, 5, 5, 2, 7, 6, 6, 9, 2, 8, 3, 5, 2, 2, 5, 6, 0, 8, 2, 9, 2, 8, 8, 8, 8, 7, 4, 9, 3, 0, 6, 6, 3, 2, 1, 3, 2, 2, 9, 3, 0, 0, 5, 7, 8, 1, 4, 4, 6, 0, 2, 9, 1, 4, 7, 4, 7, 3, 9, 8, 8, 4, 7, 1, 2, 1, 2, 2, 3, 7, 3, 2, 3, 9, 1, 7, 4, 0, 3, 5, 5, 8, 6, 3, 2, 6, 7, 6, 6, 3, 2, 7, 9, 1, 1, 7, 4, 6, 4, 9, 5, 1, 3, 3, 4, 7, 8, 9, 1, 1, 5, 9, 1, 4, 4, 5, 4, 0, 6, 2, 2, 3, 1, 5, 1, 2, 0, 3, 8, 1, 2, 6, 7, 1, 6, 2, 3, 9, 0, 1, 2, 2, 0, 8, 9, 9, 0, 2, 5, 1, 9, 7, 8, 1, 0, 4, 1, 7, 9, 0, 4, 2, 6, 8, 1, 3, 7, 5, 4, 4, 1, 8, 1, 3, 8, 1, 2, 5, 8, 0, 6, 2, 1, 1, 2, 1, 5, 3, 4, 6, 9, 5, 0, 9, 2, 2, 4, 8, 2, 1, 7, 2, 4, 9, 4, 4, 0, 3, 9, 2, 2, 3, 3, 8, 3, 5, 7, 3, 5, 8, 1, 2, 4, 4, 6, 4, 9, 5, 1, 0, 6, 9, 5, 9, 5, 9, 7, 3, 8, 0, 3, 7, 1, 3, 6, 7, 8, 5, 9, 7, 9, 6, 9, 6, 3, 7, 4, 6, 5, 3, 5, 4, 7, 8, 7, 8, 0, 7, 6, 8, 8, 7, 3, 3, 1, 9, 5, 2, 7, 3, 5, 1, 1, 2, 1, 4, 7, 4, 7, 5, 4, 5, 4, 0, 8, 3, 6, 9, 6, 0, 2, 7, 4, 4, 4, 4, 6, 6, 4, 7, 9, 3, 4, 5, 5, 8, 7, 3, 7, 2, 7, 0, 2, 4, 1, 1, 6, 6, 9, 2, 8, 7, 2, 0, 1, 5, 0, 9, 1, 7, 0, 6, 0, 8, 6, 8, 1, 8, 0, 3, 3, 7, 2, 3, 6, 2, 1, 6, 1, 1, 3, 7, 9, 0, 8, 0, 5, 4, 0, 2, 8, 2, 2, 9, 8, 4, 0, 4, 5, 8, 5, 1, 2, 1, 3, 1, 7, 9, 5, 7, 2, 0, 5, 8, 8, 6, 2, 5, 4, 1, 9, 2, 1, 5, 8, 1, 0, 2, 4, 4, 3, 6, 8, 8, 2, 4, 0, 5, 0, 4, 4, 7, 9, 3, 4, 1, 5, 9, 7, 3, 5, 8, 8, 0, 5, 3, 3, 6, 6, 0, 1, 6, 0, 3, 5, 4, 4, 1, 2, 9, 1, 4, 6, 9, 9, 3, 9, 8, 4, 4, 3, 1, 3, 1, 3, 8, 7, 9, 4, 8, 8, 7, 9, 7, 1, 4, 5, 6, 0, 5, 2, 2, 2, 1, 5, 5, 2, 4, 9, 6, 2, 7, 7, 2, 2, 1, 1, 2, 8, 3, 7, 2, 4, 1, 7, 1, 7, 6, 7, 8, 2, 7, 3, 1, 7, 5, 8, 2, 6, 2, 2, 5, 6, 6, 0, 9, 2, 4, 3, 3, 9, 7, 6, 6, 8, 0, 4, 1, 3, 8, 3, 9, 1, 8, 0, 6, 7, 2, 1, 0, 5, 5, 2, 0, 2, 2, 0, 2, 4, 9, 8, 0, 9, 9, 4, 6, 5, 4, 9, 1, 8, 3, 4, 9, 9, 1, 2, 2, 8, 1, 9, 6, 4, 0, 9, 4, 8, 3, 8, 6, 0, 2, 5, 1, 9, 6, 2, 9, 4, 0, 9, 6, 0, 6, 2, 5, 4, 2, 3, 8, 4, 5, 5, 0, 3, 8, 5, 3, 5, 8, 6, 5, 7, 6, 3, 3, 9, 6, 1, 1, 2, 9, 0, 4, 3, 3, 6, 9, 5, 7, 3, 7, 7, 7, 8, 1, 9, 8, 3, 0, 7, 2, 7, 9, 4, 5, 4, 9, 3, 2, 1, 4, 0, 2, 3, 7, 5, 7, 8, 8, 5, 0, 1, 1, 4, 7, 3, 9, 0, 0, 0, 6, 6, 2, 3, 7, 8, 4, 7, 7, 9, 2, 4, 1, 6, 5, 2, 4, 9, 9, 1, 8, 4, 0, 9, 8, 4, 8, 7, 7, 0, 7, 8, 8, 6, 0, 4, 8, 8, 2, 4, 7, 6, 6, 6, 4, 7, 1, 8, 8, 2, 3, 6, 3, 0, 0, 3, 7, 6, 9, 7, 9, 9, 5, 4, 3, 3, 6, 1, 2, 3, 7, 3, 3, 6, 0, 3, 3, 8, 4, 3, 6, 3, 5, 0, 2, 0, 9, 0, 7, 4, 6, 9, 3, 5, 1, 9, 6, 1, 4, 5, 4, 5, 0, 5, 9, 5, 2, 1, 2, 9, 1, 9, 9, 4, 0, 8, 4, 5, 2, 9, 2, 1, 2, 1, 7, 3, 6, 8, 8, 4, 9, 1, 9, 8, 5, 7, 5, 1, 1, 8, 6, 5, 2, 4, 4, 7, 2, 3, 5, 6, 8, 8, 6, 2, 3, 1, 0, 5, 8, 9, 2, 9, 6, 7, 0, 4, 8, 7, 1, 7, 4, 1, 0, 9, 7, 2, 0, 0, 9, 1, 7, 0, 7, 8, 4, 7, 2, 0, 4, 6, 0, 3, 1, 1, 3, 3, 9, 6, 7, 4, 1, 5, 3, 0, 8, 7, 3, 9, 6, 9, 3, 5, 0, 2, 7, 4, 5, 1, 7, 5, 8, 0, 8, 8, 1, 5, 0, 3, 0, 3, 1, 4, 0, 3, 7, 2, 7, 1, 8, 0, 7, 0, 4, 3, 1, 9, 8, 7, 7, 1, 4, 9, 9, 3, 8, 1, 7, 9, 0, 2, 0, 3, 3, 7, 6, 9, 2, 3, 3, 7, 7, 0, 0, 7, 5, 2, 9, 8, 7, 4, 4, 2, 6, 6, 1, 9, 6, 8, 2, 9, 0, 8, 3, 1, 1, 6, 3, 5, 1, 1, 1, 3, 1, 2, 3, 0, 2, 0, 1, 3, 5, 5, 7, 4, 8, 9, 6, 9, 6, 8, 3, 6, 6, 8, 5, 1, 4, 2, 4, 4, 5, 1, 1, 9, 0, 2, 4, 9, 5, 7, 1, 8, 3, 5, 6, 9, 8, 7, 1, 1, 6, 7, 6, 3, 2, 2, 0, 8, 9, 2, 5, 1, 0, 8, 1, 4, 5, 7, 9, 6, 9, 0, 6, 1, 5, 5, 8, 3, 8, 2, 6, 5, 0, 7, 4, 6, 1, 3, 4, 7, 3, 2, 3, 4, 2, 5, 2, 7, 1, 7, 2, 6, 4, 1, 5, 7, 8, 6, 0, 1, 8, 2, 5, 7, 7, 6, 9, 3, 5, 8, 4, 2, 4, 0, 8, 8, 3, 4, 9, 2, 7, 5, 8, 6, 5, 6, 0, 8, 6, 7, 3, 6, 4, 9, 4, 6, 5, 3, 2, 4, 1, 0, 1, 4, 6, 2, 9, 1, 1, 0, 6, 3, 9, 5, 6, 5, 6, 5, 3, 4, 6, 4, 3, 9, 1, 3, 4, 1, 9, 1, 7, 1, 1, 9, 3, 5, 4, 0, 7, 3, 6, 1, 7, 5, 5, 3, 3, 0, 1, 3, 7, 5, 8, 6, 5, 1, 0, 4, 7, 3, 4, 6, 7, 9, 8, 1, 4, 4, 9, 2, 8, 6, 2, 7, 0, 0, 6, 7, 5, 8, 6, 0, 9, 3, 9, 1, 3, 5, 4, 3, 3, 5, 5, 6, 3, 0, 2, 3, 4, 2, 3, 0, 9, 9, 4, 7, 2, 1, 4, 7, 0, 6, 0, 8, 5, 2, 8, 5, 7, 3, 0, 8, 2, 7, 2, 8, 2, 5, 5, 7, 6, 4, 0, 8, 4, 8, 2, 7, 4, 5, 2, 0, 3, 9, 9, 6, 7, 2, 5, 1, 1, 1, 2, 3, 6, 7, 8, 7, 6, 4, 8, 9, 4, 8, 6, 3, 8, 3, 1, 0, 6, 2, 2, 5, 6, 9, 5, 8, 1, 4, 1, 7, 8, 4, 6, 1, 8, 4, 3, 1, 2, 8, 0, 8, 5, 9, 2, 4, 2, 0, 2, 7, 0, 9, 0, 2, 5, 7, 6, 7, 9, 4, 2, 6, 2, 4, 4, 8, 0, 4, 4, 5, 8, 0, 6, 8, 9, 8, 5, 6, 9, 0, 4, 8, 7, 1, 3, 4, 6, 8, 0, 9, 1, 3, 3, 6, 9, 8, 7, 1, 0, 5, 7, 1, 7, 5, 2, 7, 9, 1, 8, 5, 2, 4, 9, 4, 7, 2, 2, 3, 4, 9, 1, 9, 2, 1, 7, 9, 4, 4, 1, 6, 7, 2, 7, 8, 0, 1, 9, 7, 1, 1, 7, 5, 3, 3, 5, 1, 3, 7, 6, 1, 3, 8, 7, 5, 9, 0, 0, 0, 2, 8, 8, 2, 3, 7, 1, 3, 0, 3, 4, 4, 3, 8, 9, 2, 3, 9, 7, 1, 1, 7, 0, 4, 9, 6, 5, 9, 1, 7, 0, 2, 0, 0, 4, 6, 7, 0, 7, 1, 4, 6, 4, 5, 4, 9, 9, 1, 7, 9, 5, 3, 3, 8, 2, 3, 6, 2, 2, 1, 1, 1, 1, 1, 6, 9, 8, 4, 3, 7, 1, 6, 4, 5, 0, 4, 7, 4, 2, 4, 0, 7, 0, 1, 9, 8, 8, 6, 0, 0, 4, 1, 6, 8, 2, 2, 3, 8, 4, 8, 2, 2, 1, 7, 5, 4, 4, 0, 4, 3, 9, 7, 3, 1, 0, 1, 2, 5, 9, 2, 1, 0, 1, 8, 9, 1, 6, 8, 3, 8, 9, 3, 6, 2, 8, 3, 2, 2, 1, 0, 4, 2, 9, 2, 4, 3, 7, 9, 1, 5, 2, 4, 9, 0, 3, 8, 5, 3, 6, 0, 9, 4, 6, 2, 5, 0, 2, 7, 4, 6, 6, 8, 6, 6, 8, 6, 9, 1, 7, 2, 5, 9, 9, 0, 7, 2, 7, 6, 7, 0, 6, 5, 4, 4, 7, 2, 0, 9, 9, 2, 2, 9, 4, 4, 2, 3, 3, 2, 1, 7, 0, 7, 6, 4, 1, 3, 8, 7, 4, 5, 9, 2, 5, 1, 8, 7, 3, 7, 1, 5, 5, 0, 9, 1, 4, 0, 6, 3, 3, 6, 0, 4, 9, 7, 5, 1, 6, 8, 9, 5, 5, 7, 9, 3, 8, 3, 8, 1, 5, 3, 5, 0, 5, 5, 3, 8, 6, 7, 7, 7, 3, 7, 0, 5, 9, 0, 2, 5, 5, 3, 1, 7, 7, 8, 6, 5, 9, 3, 8, 9, 5, 3, 7, 9, 1, 7, 0, 0, 3, 7, 2, 5, 8, 1, 8, 6, 2, 9, 5, 7, 5, 2, 8, 6, 2, 5, 1, 4, 8, 4, 5, 8, 3, 0, 6, 2, 7, 3, 3, 2, 1, 0, 7, 3, 4, 0, 3, 9, 3, 2, 8, 9, 0, 3, 8, 0, 7, 6, 5, 4, 7, 3, 0, 0, 8, 6, 2, 5, 1, 1, 0, 0, 4, 4, 0, 1, 2, 3, 2, 7, 7, 8, 5, 2, 5, 7, 6, 9, 1, 4, 1, 6, 4, 2, 4, 3, 5, 4, 3, 9, 5, 0, 1, 5, 3, 8, 9, 1, 9, 7, 9, 5, 5, 2, 7, 4, 6, 0, 1, 1, 1, 0, 4, 4, 7, 6, 3, 0, 0, 4, 3, 0, 6, 1, 9, 6, 1, 3, 8, 1, 2, 5, 6, 2, 7, 3, 6, 0, 1, 9, 7, 6, 6, 8, 9, 2, 9, 5, 8, 3, 1, 0, 0, 7, 6, 6, 2, 1, 6, 9, 3, 1, 8, 6, 9, 0, 6, 0, 0, 0, 6, 3, 5, 9, 3, 9, 5, 5, 8, 5, 3, 0, 4, 0, 2, 9, 6, 8, 2, 3, 1, 2, 1, 1, 5, 6, 9, 8, 0, 6, 6, 5, 5, 3, 8, 6, 2, 1, 4, 5, 4, 3, 7, 8, 3, 0, 9, 3, 5, 1, 1, 0, 4, 4, 7, 0, 1, 7, 0, 1, 6, 1, 4, 5, 6, 6, 5, 7, 8, 4, 4, 7, 2, 5, 3, 7, 0, 7, 7, 9, 6, 4, 2, 8, 5, 7, 8, 3, 9, 5, 8, 9, 9, 8, 6, 2, 8, 9, 2, 3, 6, 1, 1, 8, 9, 3, 4, 0, 7, 9, 6, 7, 1, 4, 1, 3, 4, 9, 3, 1, 4, 7, 7, 4, 7, 2, 9, 3, 0, 8, 0, 8, 4, 0, 4, 4, 1, 5, 2, 8, 3, 4, 9, 5, 2, 8, 1, 5, 3, 7, 9, 4, 2, 5, 6, 2, 5, 9, 3, 5, 9, 3, 1, 9, 1, 3, 0, 6, 9, 8, 4, 0, 4, 7, 2, 9, 0, 1, 0, 3, 1, 6, 5, 8, 1, 5, 3, 5, 0, 3, 5, 5, 9, 2, 8, 7, 0, 4, 9, 1, 9, 7, 7, 5, 5, 2, 0, 9, 1, 8, 6, 2, 3, 9, 6, 2, 1, 9, 1, 3, 5, 5, 0, 3, 8, 3, 3, 7, 6, 5, 0, 1, 4, 0, 6, 9, 8, 1, 2, 1, 9, 5, 9, 7, 3, 7, 8, 0, 1, 3, 0, 4, 6, 1, 0, 2, 5, 8, 4, 4, 1, 1, 5, 4, 6, 6, 0, 6, 9, 2, 6, 2, 7, 1, 7, 9, 4, 0, 0, 3, 8, 2, 2, 3, 1, 6, 0, 5, 7, 7, 9, 2, 6, 7, 1, 7, 6, 6, 8, 8, 4, 6, 8, 4, 1, 2, 8, 2, 3, 9, 4, 0, 3, 7, 3, 2, 3, 3, 7, 3, 4, 0, 6, 2, 0, 8, 1, 5, 3, 5, 4, 1, 7, 1, 5, 7, 5, 7, 3, 2, 2, 7, 3, 7, 3, 7, 8, 5, 4, 5, 2, 9, 6, 5, 3, 6, 7, 4, 1, 7, 1, 5, 2, 3, 6, 3, 1, 4, 2, 6, 7, 4, 3, 8, 0, 6, 2, 1, 6, 5, 3, 9, 1, 9, 3, 2, 1, 8, 4, 4, 6, 5, 8, 6, 9, 7, 7, 8, 6, 9, 7, 3, 9, 4, 0, 5, 9, 6, 4, 1, 2, 3, 0, 0, 2, 6, 6, 5, 7, 0, 8, 6, 4, 7, 9, 0, 7, 3, 4, 2, 1, 8, 8, 5, 9, 2, 7, 1, 8, 8, 8, 2, 7, 6, 0, 1, 2, 7, 1, 0, 8, 3, 6, 0, 5, 3, 6, 2, 8, 7, 0, 1, 4, 2, 1, 1, 4, 4, 4, 4, 7, 1, 6, 2, 9, 9, 0, 0, 1, 8, 8, 4, 3, 4, 2, 0, 6, 1, 6, 1, 2, 2, 2, 1, 2, 3, 7, 8, 1, 0, 0, 2, 1, 6, 6, 0, 1, 6, 2, 5, 1, 7, 4, 8, 2, 1, 4, 3, 8, 3, 9, 9, 4, 8, 3, 4, 7, 2, 7, 5, 7, 0, 4, 3, 3, 2, 6, 7, 6, 0, 0, 6, 7, 7, 0, 5, 5, 8, 1, 0, 7, 0, 2, 8, 1, 5, 0, 8, 8, 0, 3, 2, 7, 7, 2, 6, 4, 7, 5, 5, 5, 2, 9, 2, 8, 4, 6, 8, 6, 5, 0, 0, 8, 7, 6, 1, 7, 1, 1, 2, 7, 4, 0, 0, 7, 7, 6, 3, 8, 6, 4, 2, 0, 9, 4, 0, 5, 7, 8, 2, 7, 4, 7, 1, 1, 3, 6, 6, 2, 9, 1, 9, 4, 8, 3, 6, 9, 5, 9, 6, 2, 4, 6, 7, 7, 0, 6, 6, 9, 4, 8, 3, 5, 3, 4, 9, 0, 0, 5, 2, 5, 0, 7, 1, 1, 1, 0, 7, 6, 7, 9, 6, 6, 4, 1, 4, 3, 1, 1, 2, 2, 4, 1, 0, 8, 7, 6, 3, 4, 0, 0, 6, 3, 3, 0, 7, 1, 7, 1, 1, 3, 1, 0, 9, 9, 7, 5, 4, 1, 4, 8, 9, 5, 3, 5, 1, 9, 8, 2, 3, 3, 9, 9, 0, 1, 0, 2, 9, 3, 9, 3, 3, 6, 2, 4, 9, 8, 3, 7, 4, 0, 4, 7, 8, 4, 9, 8, 1, 9, 7, 5, 9, 2, 8, 2, 2, 0, 2, 2, 3, 8, 4, 6, 8, 4, 8, 2, 4, 6, 7, 9, 3, 3, 9, 4, 3, 1, 4, 4, 7, 0, 5, 9, 6, 0, 4, 4, 4, 4, 6, 1, 2, 3, 3, 6, 4, 5, 9, 6, 8, 5, 6, 0, 5, 6, 4, 1, 8, 6, 5, 2, 5, 4, 5, 5, 4, 7, 7, 0, 7, 8, 2, 2, 3, 7, 0, 1, 8, 0, 7, 1, 9, 8, 7, 5, 5, 9, 1, 7, 5, 4, 3, 1, 2, 2, 1, 6, 6, 7, 1, 1, 4, 0, 7, 4, 2, 4, 0, 6, 4, 7, 6, 9, 5, 3, 4, 6, 5, 0, 1, 8, 8, 2, 8, 3, 5, 7, 8, 0, 8, 5, 7, 1, 1, 0, 1, 3, 7, 8, 5, 0, 7, 1, 1, 0, 1, 1, 4, 5, 2, 7, 6, 2, 3, 0, 2, 8, 5, 9, 6, 9, 7, 2, 1, 3, 6, 4, 1, 8, 2, 4, 0, 5, 1, 0, 2, 2, 6, 4, 4, 3, 9, 6, 1, 6, 5, 7, 9, 2, 0, 2, 6, 0, 1, 4, 3, 5, 2, 8, 8, 0, 8, 8, 9, 0, 9, 6, 7, 6, 3, 9, 3, 4, 7, 7, 7, 4, 9, 0, 6, 4, 8, 4, 2, 7, 2, 8, 1, 0, 0, 7, 8, 3, 3, 3, 1, 3, 7, 6, 1, 3, 1, 6, 6, 5, 7, 4, 7, 5, 9, 5, 8, 4, 9, 9, 1, 6, 5, 0, 1, 3, 7, 0, 3, 4, 8, 2, 2, 0, 2, 5, 1, 5, 1, 6, 8, 8, 9, 1, 2, 1, 3, 5, 1, 0, 9, 4, 4, 8, 3, 2, 5, 9, 7, 6, 6, 2, 0, 0, 0, 5, 8, 1, 1, 5, 3, 3, 8, 5, 1, 8, 2, 0, 4, 9, 9, 6, 2, 3, 3, 5, 6, 4, 8, 0, 9, 2, 8, 3, 6, 7, 5, 1, 2, 9, 4, 9, 1, 2, 8, 6, 0, 7, 0, 9, 1, 1, 0, 7, 5, 9, 9, 1, 9, 5, 9, 2, 5, 0, 4, 1, 0, 8, 9, 0, 8, 9, 8, 9, 4, 2, 5, 7, 9, 8, 9, 8, 0, 9, 9, 6, 8, 9, 9, 5, 9, 8, 6, 1, 0, 3, 3, 5, 2, 1, 6, 5, 0, 2, 8, 3, 5, 6, 2, 3, 0, 2, 2, 6, 4, 3, 5, 5, 1, 7, 2, 1, 6, 9, 1, 9, 9, 5, 5, 1, 6, 2, 2, 8, 6, 7, 1, 4, 6, 0, 6, 0, 5, 3, 2, 2, 3, 6, 8, 9, 8, 5, 3, 8, 5, 4, 5, 2, 0, 5, 6, 3, 2, 8, 3, 9, 9, 5, 7, 9, 4, 6, 7, 1, 3, 1, 3, 6, 6, 0, 9, 0, 1, 9, 4, 2, 8, 8, 0, 1, 6, 9, 7, 5, 3, 4, 7, 4, 9, 9, 4, 3, 6, 3, 1, 1, 7, 6, 9, 1, 8, 4, 1, 1, 9, 9, 4, 3, 6, 8, 1, 6, 0, 4, 1, 3, 7, 7, 4, 9, 5, 1, 0, 0, 1, 1, 6, 2, 1, 9, 8, 4, 0, 3, 6, 4, 9, 0, 7, 1, 6, 5, 7, 5, 2, 5, 1, 8, 5, 4, 7, 0, 6, 7, 0, 2, 5, 8, 1, 0, 4, 5, 7, 1, 8, 5, 1, 3, 0, 0, 6, 0, 7, 3, 1, 8, 3, 9, 7, 0, 0, 8, 9, 5, 9, 8, 3, 2, 7, 2, 9, 7, 2, 1, 1, 3, 7, 5, 3, 1, 9, 8, 2, 2, 2, 8, 8, 5, 7, 3, 8, 9, 8, 8, 6, 8, 2, 3, 9, 7, 5, 6, 2, 9, 2, 8, 8, 1, 6, 2, 8, 7, 9, 1, 8, 0, 1, 7, 2, 0, 7, 5, 1, 9, 0, 2, 0, 9, 8, 6, 2, 3, 0, 3, 8, 0, 2, 1, 1, 1, 1, 4, 2, 9, 7, 7, 5, 1, 1, 2, 1, 9, 9, 9, 1, 0, 2, 0, 2, 1, 1, 4, 6, 4, 1, 5, 4, 9, 7, 7, 1, 5, 6, 2, 2, 2, 8, 0, 6, 9, 6, 1, 9, 7, 7, 1, 4, 8, 5, 3, 4, 3, 4, 9, 7, 5, 0, 7, 4, 8, 8, 1, 5, 3, 9, 5, 9, 7, 6, 9, 0, 3, 6, 3, 9, 8, 2, 1, 1, 2, 8, 6, 8, 5, 5, 3, 9, 4, 9, 2, 5, 1, 5, 1, 4, 4, 1, 4, 4, 3, 5, 9, 1, 2, 2, 3, 3, 0, 2, 9, 0, 0, 9, 9, 6, 0, 9, 3, 7, 8, 4, 1, 9, 9, 7, 2, 7, 9, 9, 5, 9, 5, 1, 1, 8, 3, 5, 1, 9, 5, 3, 5, 4, 9, 5, 9, 3, 1, 9, 0, 9, 7, 5, 4, 9, 2, 0, 1, 0, 5, 1, 4, 9, 3, 3, 6, 1, 5, 2, 5, 2, 2, 0, 9, 2, 6, 6, 0, 1, 2, 0, 3, 0, 2, 5, 5, 7, 9, 5, 5, 0, 8, 9, 5, 0, 3, 2, 5, 9, 0, 8, 8, 4, 5, 8, 8, 4, 5, 4, 8, 5, 4, 9, 2, 2, 1, 2, 6, 8, 8, 7, 0, 3, 6, 6, 4, 3, 8, 8, 7, 2, 2, 0, 0, 9, 3, 9, 9, 1, 9, 8, 6, 6, 4, 2, 6, 9, 2, 8, 5, 4, 5, 7, 9, 9, 9, 2, 1, 8, 3, 4, 0, 7, 8, 3, 9, 3, 4, 6, 5, 6, 2, 3, 9, 2, 6, 0, 0, 6, 1, 2, 8, 7, 9, 8, 2, 0, 4, 7, 7, 5, 0, 5, 6, 4, 6, 7, 4, 3, 0, 7, 5, 0, 7, 4, 2, 0, 8, 9, 9, 4, 2, 4, 6, 7, 8, 7, 6, 9, 4, 1, 3, 7, 3, 0, 8, 7, 7, 6, 1, 3, 9, 2, 2, 9, 2, 1, 8, 3, 2, 9, 6, 8, 4, 0, 1, 2, 8, 4, 5, 2, 7, 8, 1, 1, 3, 0, 3, 5, 7, 0, 3, 1, 9, 3, 6, 3, 1, 7, 7, 3, 0, 8, 4, 8, 2, 6, 5, 2, 9, 7, 3, 9, 0, 9, 9, 6, 4, 2, 9, 7, 2, 1, 1, 6, 7, 4, 7, 5, 9, 6, 8, 2, 1, 2, 4, 5, 7, 6, 1, 3, 2, 5, 9, 9, 3, 6, 1, 1, 4, 6, 9, 7, 2, 1, 5, 1, 4, 6, 3, 8, 1, 1, 0, 3, 1, 6, 8, 4, 9, 0, 7, 3, 0, 2, 9, 0, 6, 6, 6, 3, 6, 7, 7, 2, 8, 6, 0, 8, 3, 0, 2, 9, 8, 3, 2, 5, 3, 9, 8, 0, 0, 1, 9, 5, 1, 3, 9, 6, 0, 1, 4, 1, 7, 1, 2, 3, 7, 9, 7, 4, 9, 9, 3, 9, 2, 8, 2, 7, 1, 8, 0, 9, 1, 0, 1, 7, 7, 9, 6, 9, 9, 9, 2, 1, 6, 1, 3, 5, 7, 1, 9, 7, 6, 4, 5, 7, 6, 6, 9, 9, 6, 3, 6, 2, 9, 8, 1, 2, 2, 5, 5, 2, 3, 7, 2, 1, 0, 1, 0, 4, 5, 2, 8, 2, 8, 3, 5, 1, 7, 7, 1, 1, 2, 9, 7, 8, 4, 0, 5, 0, 7, 8, 8, 4, 7, 7, 8, 5, 8, 4, 9, 8, 1, 3, 8, 0, 3, 1, 7, 1, 5, 5, 1, 6, 5, 7, 4, 9, 3, 5, 4, 7, 1, 2, 0, 8, 1, 6, 0, 7, 3, 4, 7, 3, 9, 6, 0, 8, 6, 4, 8, 7, 7, 9, 3, 8, 6, 9, 7, 2, 3, 4, 0, 2, 1, 0, 3, 5, 5, 7, 2, 4, 6, 7, 2, 8, 3, 0, 8, 7, 8, 4, 0, 8, 4, 4, 5, 8, 5, 6, 6, 3, 0, 9, 3, 7, 6, 8, 9, 3, 4, 9, 5, 8, 9, 1, 2, 8, 8, 6, 8, 1, 3, 7, 9, 0, 1, 1, 4, 7, 0, 8, 1, 7, 4, 5, 7, 1, 2, 1, 1, 3, 9, 6, 2, 1, 2, 6, 0, 7, 6, 6, 9, 3, 7, 0, 5, 2, 8, 0, 5, 4, 3, 8, 4, 6, 6, 2, 7, 9, 5, 1, 3, 2, 4, 3, 6, 1, 9, 4, 4, 7, 6, 5, 4, 1, 9, 9, 2, 7, 8, 0, 1, 3, 6, 1, 3, 4, 1, 1, 1, 5, 6, 0, 7, 0, 7, 2, 3, 2, 5, 2, 2, 9, 4, 9, 8, 1, 2, 1, 6, 1, 2, 7, 8, 0, 0, 0, 8, 2, 2, 9, 2, 2, 3, 9, 9, 2, 7, 5, 1, 3, 4, 9, 4, 1, 8, 5, 6, 2, 8, 3, 1, 2, 8, 4, 9, 9, 3, 7, 0, 7, 7, 2, 3, 2, 4, 0, 3, 9, 9, 8, 4, 1, 0, 6, 0, 9, 6, 8, 6, 1, 1, 9, 8, 9, 2, 3, 5, 5, 9, 4, 2, 1, 9, 4, 3, 9, 6, 0, 4, 0, 6, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 3, 4, 7, 8, 6, 3, 4, 0, 9, 7, 1, 9, 3, 8, 4, 7, 3, 0, 9, 1, 4, 5, 4, 6, 2, 0, 6, 2, 1, 1, 1, 1, 7, 2, 4, 7, 5, 2, 9, 4, 5, 8, 4, 2, 9, 7, 0, 0, 7, 5, 1, 1, 7, 6, 6, 6, 8, 2, 2, 7, 7, 4, 0, 2, 4, 2, 1, 8, 9, 6, 1, 0, 5, 9, 6, 9, 8, 0, 3, 0, 8, 3, 9, 6, 3, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 4, 8, 7, 4, 7, 7, 3, 9, 8, 8, 3, 1, 5, 8, 2, 7, 4, 2, 1, 5, 4, 5, 5, 8, 6, 4, 4, 4, 1, 8, 7, 5, 5, 1, 8, 9, 1, 3, 6, 3, 3, 2, 2, 6, 9, 9, 6, 5, 5, 3, 3, 8, 1, 6, 5, 6, 8, 1, 9, 7, 6, 8, 3, 7, 4, 7, 0, 9, 0, 0, 3, 7, 9, 3, 0, 2, 0, 1, 0, 1, 0, 4, 0, 1, 0, 4, 7, 9, 6, 2, 6, 2, 2, 9, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 0, 5, 6, 6, 0, 8, 0, 2, 3, 7, 9, 4, 7, 1, 9, 1, 7, 1, 4, 0, 0, 4, 1, 7, 5, 7, 1, 3, 3, 3, 6, 6, 9, 7, 4, 3, 0, 2, 5, 2, 6, 0, 8, 9, 4, 3, 5, 4, 8, 1, 5, 9, 0, 6, 4, 3, 6, 3, 3, 8, 1, 4, 7, 5, 7, 2, 2, 0, 0, 1, 7, 7, 9, 5, 9, 8, 9, 6, 8, 8, 2, 3, 6, 1, 2, 9, 8, 9, 5, 2, 6, 2, 4, 8, 4, 6, 5, 0, 1, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 4, 2, 0, 9, 0, 1, 5, 8, 8, 0, 2, 7, 8, 4, 4, 6, 1, 0, 4, 5, 3, 9, 4, 2, 0, 5, 0, 1, 3, 2, 9, 1, 6, 0, 1, 1, 8, 0, 4, 7, 7, 6, 3, 6, 0, 7, 3, 5, 4, 2, 4, 1, 8, 3, 5, 6, 7, 0, 6, 7, 1, 2, 5, 8, 1, 9, 3, 8, 2, 8, 7, 6, 7, 1, 4, 6, 2, 9, 3, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 0, 1, 2, 8, 9, 1, 4, 0, 9, 5, 0, 8, 0, 7, 7, 1, 1, 2, 9, 3, 6, 7, 2, 3, 8, 1, 2, 9, 8, 8, 7, 1, 7, 1, 1, 0, 3, 4, 2, 6, 4, 7, 4, 2, 7, 4, 9, 1, 0, 6, 8, 5, 5, 5, 3, 5, 9, 7, 4, 8, 5, 9, 6, 9, 3, 0, 3, 8, 9, 1, 8, 1, 6, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 3, 2, 9, 3, 2, 1, 4, 5, 5, 3, 3, 2, 1, 3, 9, 7, 2, 5, 2, 8, 9, 1, 8, 8, 7, 8, 1, 0, 0, 7, 7, 8, 7, 5, 0, 6, 1, 5, 7, 4, 6, 1, 2, 5, 0, 7, 9, 9, 0, 3, 8, 4, 4, 8, 1, 8, 6, 5, 9, 0, 0, 0, 3, 7, 1, 6, 4, 2, 6, 6, 0, 4, 5, 4, 1, 3, 8, 6, 3, 9, 9, 5, 9, 3, 7, 8, 5, 6, 4, 7, 6, 2, 2, 0, 9, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 7, 5, 6, 0, 1, 2, 3, 4, 5, 6, 8, 7, 1, 3, 2, 2, 0, 7, 5, 9, 9, 6, 0, 9, 4, 1, 3, 2, 1, 2, 3, 8, 3, 2, 6, 5, 6, 8, 2, 7, 4, 8, 1, 8, 0, 5, 3, 9, 4, 1, 9, 2, 1, 9, 6, 7, 9, 0, 4, 6, 1, 7, 3, 8, 7, 2, 9, 6, 5, 8, 3, 9, 0, 5, 7, 1, 6, 1, 0, 9, 3, 3, 4, 4, 0, 6, 2, 5, 4, 2, 3, 4, 6, 0, 0, 2, 0, 1, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 7, 1, 3, 7, 5, 2, 8, 0, 7, 5, 9, 9, 0, 9, 1, 1, 5, 8, 8, 6, 3, 2, 1, 8, 3, 2, 6, 5, 6, 0, 4, 1, 0, 5, 3, 1, 9, 2, 1, 9, 6, 0, 4, 6, 1, 7, 3, 8, 7, 2, 9, 6, 5, 8, 3, 5, 7, 1, 6, 1, 0, 9, 6, 2, 5, 4, 2, 3, 4, 4, 6, 0, 0, 2, 0, 1, 2, 3, 9, 3, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 8, 4, 5, 6, 7, 8, 9, 8, 6, 5, 0, 6, 8, 9, 4, 1, 9, 3, 8, 0, 4, 8, 9, 1, 4, 0, 5, 5, 2, 1, 5, 4, 0, 7, 6, 0, 1, 7, 0, 6, 8, 9, 9, 1, 7, 9, 8, 6, 0, 8, 1, 7, 7, 1, 3, 2, 3, 1, 4, 2, 0, 0, 7, 8, 4, 6, 4, 9, 1, 8, 4, 7, 2, 5, 6, 3, 6, 9, 6, 3, 2, 2, 4, 6, 9, 0, 2, 5, 5, 1, 3, 3, 9, 7, 8, 7, 2, 2, 5, 7, 9, 8, 2, 1, 3, 1, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 6, 5, 3, 0, 7, 0, 4, 1, 4, 3, 6, 7, 2, 3, 1, 2, 1, 2, 9, 6, 0, 1, 3, 0, 2, 7, 5, 7, 6, 2, 9, 1, 9, 0, 6, 0, 6, 0, 2, 0, 6, 1, 5, 8, 4, 3, 0, 1, 5, 4, 4, 8, 5, 7, 5, 7, 8, 3, 4, 8, 8, 5, 2, 9, 7, 1, 3, 8, 1, 0, 7, 5, 3, 6, 9, 4, 7, 7, 9, 9, 3, 4, 4, 3, 8, 6, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 8, 3, 9, 5, 5, 2, 6, 8, 4, 9, 1, 7, 1, 2, 3, 5, 9, 6, 9, 1, 1, 1, 2, 9, 5, 6, 8, 1, 2, 0, 7, 7, 5, 8, 2, 9, 8, 9, 0, 4, 6, 7, 1, 3, 4, 5, 6, 0, 3, 6, 8, 7, 0, 4, 2, 7, 4, 7, 5, 4, 3, 4, 2, 8, 1, 5, 1, 2, 0, 2, 5, 6, 4, 3, 0, 0, 0, 3, 3, 5, 7, 0, 6, 4, 8, 8, 6, 3, 4, 6, 9, 9, 8, 2, 7, 7, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 2, 1, 7, 2, 5, 0, 8, 0, 2, 7, 8, 8, 3, 6, 0, 2, 7, 6, 6, 1, 2, 8, 8, 7, 7, 4, 7, 7, 3, 7, 4, 5, 4, 3, 3, 8, 4, 1, 1, 9, 7, 4, 3, 7, 3, 3, 0, 2, 5, 5, 6, 6, 3, 5, 2, 5, 9, 9, 8, 4, 1, 0, 6, 0, 9, 6, 8, 8, 5, 6, 1, 1, 9, 8, 9, 2, 3, 5, 5, 9, 4, 2, 1, 9, 3, 9, 2, 0, 6, 0, 4, 0, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 7, 3, 0, 3, 1, 8, 7, 6, 4, 0, 2, 6, 8, 3, 2, 8, 1, 2, 0, 7, 1, 0, 4, 4, 5, 8, 0, 6, 2, 3, 1, 5, 1, 8, 5, 9, 4, 0, 7, 5, 8, 8, 3, 8, 9, 2, 6, 2, 5, 3, 1, 7, 3, 0, 1, 9, 9, 6, 0, 3, 9, 2, 8, 1, 4, 3, 5, 2, 9, 2, 5, 8, 9, 5, 0, 1, 2, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 0, 4, 5, 6, 6, 3, 4, 4, 2, 9, 1, 0, 6, 4, 9, 7, 2, 3, 3, 9, 2, 0, 9, 3, 3, 7, 1, 5, 6, 3, 1, 7, 8, 4, 0, 2, 4, 0, 2, 4, 7, 8, 0, 7, 0, 6, 9, 3, 2, 8, 6, 7, 5, 7, 5, 1, 0, 8, 1, 6, 7, 2, 9, 7, 9, 5, 8, 6, 2, 6, 2, 8, 1, 7, 5, 0, 1, 1, 3, 2, 4, 9, 1, 8, 6, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 5, 9, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 7, 8, 9, 9, 8, 9, 8, 4, 1, 7, 7, 3, 3, 7, 6, 6, 6, 1, 9, 0, 1, 7, 6, 3, 2, 1, 7, 1, 3, 9, 1, 7, 6, 8, 4, 1, 4, 3, 6, 9, 6, 1, 4, 4, 7, 2, 4, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 2, 3, 4, 7, 8, 1, 3, 5, 1, 7, 7, 2, 1, 4, 8, 3, 4, 4, 3, 9, 7, 4, 1, 2, 3, 5, 9, 1, 6, 0, 1, 0, 0, 2, 8, 7, 1, 1, 4, 0, 4, 7, 3, 6, 8, 0, 3, 7, 4, 0, 6, 9, 2, 6, 5, 8, 6, 9, 0, 4, 0, 6, 1, 9, 2, 0, 9, 5, 1, 3, 7, 6, 9, 3, 0, 2, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 7, 2, 5, 0, 8, 0, 2, 7, 8, 8, 3, 0, 6, 0, 2, 7, 6, 6, 1, 2, 8, 8, 7, 7, 4, 7, 7, 3, 7, 4, 5, 4, 3, 3, 8, 4, 5, 4, 1, 1, 9, 7, 4, 3, 7, 3, 3, 0, 2, 5, 5, 6, 3, 1, 5, 2, 5, 9, 9, 8, 4, 1, 0, 6, 0, 9, 6, 8, 8, 5, 6, 1, 1, 9, 8, 9, 2, 3, 5, 5, 9, 4, 2, 1, 9, 4, 9, 1, 3, 9, 2, 0, 6, 0, 4, 0, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 8, 0, 7, 1, 0, 7, 5, 5, 6, 9, 0, 1, 0, 0, 8, 3, 4, 3, 1, 5, 0, 0, 9, 5, 3, 4, 9, 3, 7, 6, 9, 2, 4, 5, 7, 2, 6, 4, 9, 4, 9, 4, 1, 2, 2, 5, 8, 1, 3, 2, 9, 4, 3, 8, 2, 2, 1, 2, 8, 6, 5, 1, 6, 7, 2, 1, 3, 9, 3, 8, 7, 5, 7, 0, 7, 4, 8, 8, 5, 0, 6, 6, 3, 7, 6, 9, 9, 4, 8, 4, 1, 0, 6, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 4, 0, 4, 0, 1, 7, 9, 5, 1, 4, 2, 8, 9, 4, 3, 7, 8, 2, 4, 4, 3, 3, 6, 9, 9, 5, 8, 6, 7, 0, 6, 8, 2, 6, 3, 9, 3, 2, 8, 6, 1, 7, 4, 8, 8, 9, 0, 3, 3, 9, 0, 5, 2, 9, 4, 1, 0, 3, 7, 5, 8, 7, 7, 8, 2, 9, 7, 1, 2, 6, 4, 2, 5, 2, 3, 6, 6, 5, 0, 0, 2, 8, 1, 6, 1, 0, 4, 3, 1, 6, 1, 9, 0, 1, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 4, 0, 0, 7, 2, 4, 3, 8, 6, 6, 3, 2, 6, 3, 3, 6, 1, 4, 7, 8, 0, 3, 1, 9, 0, 1, 9, 1, 2, 7, 0, 1, 3, 8, 2, 9, 2, 7, 6, 5, 5, 9, 9, 8, 2, 9, 1, 3, 2, 3, 4, 3, 1, 9, 0, 9, 3, 6, 8, 7, 0, 1, 0, 5, 8, 2, 7, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 7, 4, 8, 1, 5, 6, 5, 7, 2, 8, 6, 3, 3, 8, 6, 5, 4, 0, 9, 1, 7, 2, 9, 1, 5, 1, 3, 2, 2, 3, 0, 6, 4, 3, 7, 6, 9, 0, 4, 8, 1, 4, 0, 6, 1, 2, 6, 9, 2, 2, 3, 5, 5, 1, 0, 7, 7, 9, 6, 2, 9, 4, 7, 0, 2, 3, 4, 0, 0, 8, 8, 8, 5, 1, 3, 7, 4, 9, 8, 8, 9, 0, 9, 8, 9, 0, 2, 6, 5, 6, 7, 4, 7, 5, 4, 1, 3, 5, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 0, 1, 2, 4, 5, 6, 7, 8, 1, 7, 2, 4, 1, 4, 1, 4, 9, 6, 8, 4, 5, 3, 7, 8, 4, 3, 3, 5, 6, 7, 0, 6, 1, 6, 8, 7, 0, 1, 5, 0, 8, 5, 0, 1, 5, 8, 4, 2, 3, 9, 7, 6, 9, 1, 9, 0, 6, 7, 1, 2, 3, 9, 2, 4, 5, 5, 3, 7, 5, 3, 1, 8, 2, 2, 3, 0, 2, 9, 4, 9, 7, 0, 2, 7, 4, 9, 9, 2, 5, 9, 8, 3, 8, 6, 7, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 7, 2, 6, 5, 5, 3, 7, 8, 6, 6, 6, 6, 4, 3, 8, 8, 3, 0, 1, 9, 0, 5, 4, 1, 9, 1, 2, 7, 0, 1, 3, 8, 2, 9, 2, 7, 4, 2, 6, 5, 5, 9, 9, 1, 1, 5, 7, 6, 8, 2, 9, 4, 3, 1, 9, 0, 9, 3, 6, 8, 7, 0, 1, 0, 5, 8, 2, 7, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 1, 3, 9, 9, 8, 5, 3, 7, 0, 7, 7, 5, 7, 9, 9, 4, 7, 0, 3, 4, 1, 5, 8, 1, 4, 8, 4, 1, 8, 6, 6, 4, 6, 0, 5, 5, 3, 3, 5, 7, 2, 5, 9, 6, 9, 2, 6, 2, 1, 2, 0, 8, 3, 8, 3, 0, 8, 7, 4, 9, 5, 0, 9, 7, 0, 0, 4, 6, 0, 9, 1, 6, 2, 7, 6, 8, 3, 5, 2, 1, 8, 3, 8, 6, 1, 0, 2, 1, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 7, 6, 4, 7, 6, 2, 3, 4, 8, 7, 8, 6, 9, 8, 3, 2, 2, 8, 4, 8, 5, 6, 5, 0, 2, 0, 1, 1, 2, 9, 6, 8, 2, 1, 0, 6, 5, 2, 9, 7, 5, 3, 9, 3, 7, 1, 8, 3, 8, 1, 9, 5, 5, 0, 1, 1, 9, 8, 2, 6, 0, 4, 5, 0, 3, 1, 8, 6, 7, 5, 9, 9, 3, 0, 3, 1, 4, 4, 0, 4, 9, 0, 1, 2, 3, 5, 6, 7, 8, 0, 1, 2, 3, 5, 6, 7, 8, 9, 0, 1, 2, 3, 5, 6, 7, 8, 9, 9, 7, 0, 9, 0, 1, 5, 8, 8, 0, 9, 3, 2, 7, 8, 4, 6, 1, 0, 4, 9, 4, 2, 0, 5, 0, 1, 6, 9, 3, 2, 9, 1, 6, 0, 1, 1, 8, 7, 7, 6, 3, 6, 0, 7, 2, 4, 1, 7, 0, 6, 7, 1, 2, 5, 8, 1, 1, 2, 8, 7, 6, 8, 7, 1, 6, 2, 9, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 9, 5, 7, 0, 3, 1, 6, 8, 4, 1, 5, 6, 4, 2, 7, 8, 1, 3, 4, 3, 4, 7, 2, 0, 5, 0, 1, 9, 2, 3, 2, 3, 5, 5, 7, 8, 4, 9, 9, 7, 1, 1, 9, 0, 7, 8, 3, 4, 8, 6, 3, 8, 0, 9, 6, 2, 1, 0, 1, 0, 6, 2, 3, 8, 9, 0, 7, 2, 3, 4, 5, 5, 2, 8, 5, 4, 6, 6, 6, 7, 9, 1, 8, 2, 1, 5, 3, 4, 7, 9, 4, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 9, 0, 1, 3, 1, 5, 1, 2, 4, 9, 8, 4, 6, 8, 0, 1, 1, 9, 2, 6, 6, 8, 7, 4, 2, 9, 7, 0, 2, 1, 0, 3, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 4, 7, 8, 9, 8, 6, 5, 9, 7, 0, 2, 3, 4, 3, 8, 5, 1, 5, 2, 3, 0, 1, 2, 1, 3, 2, 6, 5, 3, 0, 7, 2, 7, 4, 6, 4, 0, 5, 9, 9, 8, 9, 5, 3, 1, 7, 4, 7, 6, 5, 4, 0, 0, 6, 6, 2, 0, 6, 3, 7, 7, 4, 4, 3, 9, 2, 8, 9, 6, 0, 9, 5, 3, 8, 8, 7, 1, 4, 0, 4, 8, 5, 2, 3, 9, 0, 1, 9, 1, 5, 1, 7, 4, 8, 6, 2, 1, 6, 8, 8, 0, 1, 2, 9, 4, 7, 8, 9, 0, 1, 2, 3, 4, 6, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 1, 4, 5, 3, 3, 0, 9, 5, 4, 3, 0, 8, 4, 6, 7, 0, 7, 7, 1, 6, 9, 1, 3, 6, 2, 3, 8, 2, 3, 8, 9, 5, 8, 8, 7, 1, 7, 1, 1, 0, 3, 4, 2, 6, 4, 7, 4, 2, 7, 4, 2, 9, 2, 7, 9, 2, 1, 6, 6, 5, 3, 4, 8, 5, 9, 6, 9, 0, 6, 3, 0, 8, 1, 6, 0, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 7, 2, 5, 1, 6, 4, 3, 9, 9, 0, 9, 7, 1, 6, 4, 3, 6, 2, 0, 9, 8, 6, 5, 7, 0, 0, 1, 7, 4, 3, 2, 4, 1, 3, 7, 6, 4, 7, 7, 7, 9, 8, 4, 3, 8, 2, 8, 3, 5, 8, 0, 5, 4, 7, 1, 3, 1, 7, 9, 6, 2, 0, 9, 1, 7, 3, 3, 9, 1, 6, 4, 3, 9, 8, 2, 1, 8, 6, 4, 1, 5, 5, 6, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 9, 7, 0, 2, 3, 4, 3, 8, 5, 1, 3, 0, 1, 2, 1, 3, 2, 0, 7, 2, 6, 4, 0, 5, 9, 9, 8, 9, 5, 3, 1, 7, 4, 7, 0, 0, 6, 6, 6, 3, 7, 4, 2, 6, 9, 8, 7, 1, 9, 0, 4, 8, 5, 2, 3, 9, 0, 1, 9, 1, 5, 1, 7, 6, 1, 2, 1, 6, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 5, 6, 7, 8, 1, 0, 4, 5, 6, 6, 3, 4, 4, 2, 8, 1, 0, 6, 4, 9, 7, 2, 9, 2, 0, 9, 3, 3, 9, 1, 5, 2, 3, 1, 6, 7, 3, 7, 8, 4, 0, 2, 4, 0, 2, 4, 7, 8, 0, 7, 0, 6, 9, 3, 2, 4, 8, 6, 0, 5, 7, 5, 1, 0, 8, 1, 6, 7, 2, 9, 7, 9, 5, 6, 5, 2, 6, 2, 8, 1, 7, 5, 5, 7, 3, 5, 0, 1, 1, 3, 8, 4, 9, 4, 5, 1, 8, 6, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 3, 2, 9, 3, 2, 1, 4, 5, 5, 2, 3, 2, 1, 3, 9, 7, 2, 1, 2, 8, 9, 1, 8, 8, 7, 8, 1, 0, 0, 6, 7, 7, 8, 7, 5, 0, 6, 1, 5, 7, 4, 6, 1, 2, 5, 0, 7, 9, 9, 0, 3, 4, 4, 8, 4, 1, 8, 6, 5, 9, 0, 0, 0, 3, 7, 1, 6, 4, 6, 0, 4, 5, 4, 1, 3, 8, 6, 3, 9, 9, 5, 9, 3, 7, 8, 5, 6, 4, 7, 6, 2, 2, 0, 9, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 4, 2, 6, 4, 7, 5, 5, 4, 7, 2, 9, 3, 9, 3, 8, 2, 0, 9, 5, 6, 0, 1, 0, 6, 5, 3, 5, 3, 8, 0, 0, 3, 4, 1, 5, 3, 0, 8, 3, 0, 6, 2, 7, 8, 1, 7, 1, 3, 8, 5, 4, 2, 0, 9, 7, 6, 7, 4, 1, 6, 2, 6, 7, 1, 9, 8, 0, 6, 9, 4, 9, 9, 6, 2, 3, 7, 1, 9, 2, 2, 5, 3, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 0, 1, 7, 8, 9, 8, 9, 2, 6, 1, 3, 5, 4, 8, 2, 6, 4, 3, 4, 5, 9, 2, 0, 3, 9, 4, 9, 7, 3, 8, 7, 4, 4, 9, 8, 5, 8, 2, 6, 6, 2, 3, 1, 3, 2, 7, 3, 1, 9, 0, 1, 1, 3, 5, 0, 7, 8, 1, 5, 1, 4, 6, 0, 0, 4, 9, 1, 6, 6, 9, 0, 7, 6, 1, 1, 0, 1, 2, 3, 4, 2, 2, 3, 4, 5, 6, 2, 0, 1, 2, 2, 8, 6, 3, 9, 2, 1, 9, 3, 9, 6, 1, 7, 2, 4, 4, 5, 7, 0, 0, 1, 6, 6, 8, 2, 7, 7, 2, 4, 2, 1, 6, 1, 0, 6, 9, 8, 3, 9, 6, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 6, 8, 9, 9, 0, 1, 2, 4, 4, 3, 7, 4, 4, 4, 0, 3, 8, 7, 5, 8, 2, 1, 7, 5, 3, 8, 5, 2, 5, 1, 1, 6, 2, 1, 3, 8, 6, 4, 2, 6, 2, 5, 5, 0, 2, 8, 0, 6, 8, 1, 7, 9, 1, 9, 2, 6, 7, 6, 6, 8, 7, 4, 9, 2, 1, 3, 3, 0, 5, 5, 8, 0, 3, 7, 9, 7, 0, 2, 7, 9, 1, 7, 8, 0, 3, 5, 3, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 7, 8, 9, 6, 4, 2, 6, 4, 7, 8, 9, 2, 9, 3, 9, 3, 0, 0, 1, 0, 4, 2, 6, 3, 5, 3, 0, 3, 4, 1, 5, 3, 0, 8, 3, 0, 6, 1, 7, 8, 0, 9, 2, 6, 7, 1, 9, 6, 9, 4, 9, 9, 6, 7, 1, 2, 5, 3, 7, 8, 0, 1, 2, 4, 5, 6, 7, 8, 9, 0, 1, 3, 4, 5, 6, 7, 3, 0, 1, 3, 4, 7, 8, 9, 7, 5, 5, 1, 9, 9, 7, 1, 0, 0, 5, 9, 7, 1, 7, 2, 2, 3, 6, 8, 3, 2, 0, 0, 6, 1, 7, 5, 8, 6, 2, 9, 4, 8, 8, 7, 1, 0, 8, 7, 7, 5, 8, 5, 3, 4, 6, 1, 1, 5, 5, 0, 7, 2, 3, 6, 4, 1, 2, 4, 1, 5, 4, 2, 0, 4, 8, 6, 1, 9, 0, 2, 5, 6, 9, 3, 6, 3, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 5, 6, 7, 8, 1, 0, 9, 5, 7, 5, 1, 8, 6, 9, 0, 4, 1, 9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 5, 9, 6, 0, 6, 5, 5, 3, 3, 3, 9, 8, 1, 1, 0, 6, 1, 0, 0, 6, 2, 1, 1, 3, 2, 7, 7, 8, 8, 7, 8, 4, 6, 0, 2, 0, 7, 0, 3, 6, 8, 7, 1, 5, 9, 9, 3, 7, 2, 4, 9, 4, 3, 6, 2, 2, 5, 3, 2, 5, 5, 9, 4, 1, 7, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 0, 1, 2, 7, 5, 3, 4, 4, 0, 0, 6, 9, 6, 6, 5, 7, 2, 3, 4, 4, 9, 1, 4, 0, 7, 9, 5, 7, 2, 3, 1, 4, 4, 0, 9, 9, 6, 1, 8, 3, 3, 7, 3, 9, 8, 8, 4, 7, 7, 6, 2, 1, 9, 8, 7, 8, 8, 7, 2, 2, 3, 9, 3, 3, 5, 5, 0, 7, 4, 5, 6, 5, 1, 4, 1, 1, 2, 8, 2, 6, 1, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 0, 6, 0, 0, 2, 3, 7, 9, 4, 7, 1, 9, 1, 7, 1, 4, 0, 0, 1, 7, 5, 7, 1, 3, 3, 3, 1, 6, 9, 7, 1, 3, 0, 2, 6, 0, 8, 9, 4, 3, 5, 4, 8, 1, 5, 9, 0, 6, 3, 3, 8, 1, 4, 7, 5, 2, 0, 0, 1, 7, 8, 9, 6, 8, 8, 2, 3, 6, 1, 2, 9, 5, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 6, 6, 7, 8, 9, 7, 4, 6, 1, 4, 0, 9, 9, 3, 7, 8, 0, 7, 5, 8, 5, 3, 2, 2, 0, 5, 8, 6, 0, 3, 8, 1, 0, 3, 0, 4, 7, 4, 9, 0, 9, 0, 7, 1, 7, 1, 6, 6, 5, 6, 0, 8, 7, 6, 4, 9, 9, 5, 3, 7, 4, 3, 0, 1, 6, 6, 1, 1, 3, 2, 1, 0, 0, 1, 2, 3, 4, 7, 8, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 0, 8, 3, 9, 5, 5, 2, 6, 8, 4, 1, 7, 1, 7, 3, 5, 6, 9, 1, 1, 1, 2, 1, 2, 0, 7, 7, 5, 8, 2, 9, 8, 6, 7, 3, 4, 6, 8, 7, 0, 4, 2, 7, 7, 5, 4, 3, 4, 2, 8, 1, 5, 1, 0, 2, 3, 3, 5, 7, 0, 6, 8, 6, 3, 9, 9, 8, 2, 7, 7, 1, 0, 1, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 7, 8, 9, 7, 8, 6, 4, 1, 9, 3, 8, 4, 4, 7, 0, 1, 9, 2, 8, 7, 8, 2, 6, 0, 6, 5, 3, 3, 8, 9, 1, 4, 0, 6, 1, 0, 0, 6, 2, 1, 1, 7, 7, 8, 4, 6, 0, 7, 0, 3, 6, 8, 7, 1, 5, 2, 4, 9, 4, 3, 6, 4, 1, 7, 2, 6, 5, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6]\n",
      "RUNTIME: 0.25m\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result_lst = []\n",
    "for i in range(10000):\n",
    "    output0 = model0(x[i].float()).item()\n",
    "    output1 = model1(x[i].float()).item()\n",
    "    output2 = model2(x[i].float()).item()\n",
    "    output3 = model3(x[i].float()).item()\n",
    "    output4 = model4(x[i].float()).item()\n",
    "    output5 = model5(x[i].float()).item()\n",
    "    output6 = model6(x[i].float()).item()\n",
    "    output7 = model7(x[i].float()).item()\n",
    "    output8 = model8(x[i].float()).item()\n",
    "    output9 = model9(x[i].float()).item()\n",
    "    max_val = max(output0,output1,output2,output3,output4,output5,output6,output7,output8,output9)\n",
    "    if(max_val == output0):\n",
    "        result_lst.append(0)\n",
    "    elif(max_val == output1):\n",
    "        result_lst.append(1)\n",
    "    elif(max_val == output2):\n",
    "        result_lst.append(2)\n",
    "    elif(max_val == output3):\n",
    "        result_lst.append(3)\n",
    "    elif(max_val == output4):\n",
    "        result_lst.append(4)\n",
    "    elif(max_val == output5):\n",
    "        result_lst.append(5)\n",
    "    elif(max_val == output6):\n",
    "        result_lst.append(6)\n",
    "    elif(max_val == output7):\n",
    "        result_lst.append(7)\n",
    "    elif(max_val == output8):\n",
    "        result_lst.append(8)\n",
    "    elif(max_val == output9):\n",
    "        result_lst.append(9)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "#     print(1/(1+np.exp(-output0)))\n",
    "#     print(1/(1+np.exp(-output1)))\n",
    "#     print(1/(1+np.exp(-output2)))\n",
    "#     print(1/(1+np.exp(-output3)))\n",
    "#     print(1/(1+np.exp(-output4)))\n",
    "#     print(1/(1+np.exp(-output5)))\n",
    "#     print(1/(1+np.exp(-output6)))\n",
    "#     print(1/(1+np.exp(-output7)))\n",
    "#     print(1/(1+np.exp(-output8)))\n",
    "#     print(1/(1+np.exp(-output9)))\n",
    "#     print(output1)\n",
    "#     print(output2)\n",
    "#     print(output3)\n",
    "#     print(output4)\n",
    "#     print(output5)\n",
    "#     print(output6)\n",
    "#     print(output7)\n",
    "#     print(output8)\n",
    "#     print(output9)\n",
    "\n",
    "    \n",
    "print(result_lst)\n",
    "\n",
    "end = time.time()\n",
    "print(\"RUNTIME:%5.2fm\"%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d000d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9843\n"
     ]
    }
   ],
   "source": [
    "match = 0\n",
    "total = 10000\n",
    "for i in range(total):\n",
    "    if(y[i]==result_lst[i]):\n",
    "            match = match+1\n",
    "print(\"Accuracy:\",match/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057bd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "# choose the training and testing datasets\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index = indices[split:], indices[:split]\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n",
    "                                           sampler = train_sampler, num_workers = num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
    "                                          sampler = valid_sampler, num_workers = num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size,\n",
    "                                         num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a293188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# define NN architecture\n",
    "class FixNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FixNet,self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 512\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(512,10)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        # self.droput = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1,28*28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        #x = self.droput(x)\n",
    "         # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        #x = self.droput(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6872a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = FixNet()\n",
    "\n",
    "newmodel.fc1.weight = torch.nn.Parameter(means[0])\n",
    "newmodel.fc1.bias = torch.nn.Parameter(means[1])\n",
    "newmodel.fc2.weight = torch.nn.Parameter(means[2])\n",
    "newmodel.fc2.bias = torch.nn.Parameter(means[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4461900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(newmodel.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc423750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.399738 \tValidation Loss: 0.001751\n",
      "Validation loss decreased (inf --> 0.001751).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.015103 \tValidation Loss: 0.001707\n",
      "Validation loss decreased (0.001751 --> 0.001707).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.894935 \tValidation Loss: 0.001492\n",
      "Validation loss decreased (0.001707 --> 0.001492).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.827743 \tValidation Loss: 0.000785\n",
      "Validation loss decreased (0.001492 --> 0.000785).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.782266 \tValidation Loss: 0.000874\n",
      "Epoch: 6 \tTraining Loss: 0.749263 \tValidation Loss: 0.001082\n",
      "Epoch: 7 \tTraining Loss: 0.724209 \tValidation Loss: 0.001696\n",
      "Epoch: 8 \tTraining Loss: 0.702961 \tValidation Loss: 0.001816\n",
      "Epoch: 9 \tTraining Loss: 0.685066 \tValidation Loss: 0.001849\n",
      "Epoch: 10 \tTraining Loss: 0.670683 \tValidation Loss: 0.001414\n",
      "Epoch: 11 \tTraining Loss: 0.658166 \tValidation Loss: 0.001880\n",
      "Epoch: 12 \tTraining Loss: 0.646750 \tValidation Loss: 0.000788\n",
      "Epoch: 13 \tTraining Loss: 0.636082 \tValidation Loss: 0.000975\n",
      "Epoch: 14 \tTraining Loss: 0.627122 \tValidation Loss: 0.000728\n",
      "Validation loss decreased (0.000785 --> 0.000728).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.618666 \tValidation Loss: 0.000783\n",
      "Epoch: 16 \tTraining Loss: 0.611070 \tValidation Loss: 0.000827\n",
      "Epoch: 17 \tTraining Loss: 0.604392 \tValidation Loss: 0.001098\n",
      "Epoch: 18 \tTraining Loss: 0.597844 \tValidation Loss: 0.001373\n",
      "Epoch: 19 \tTraining Loss: 0.591939 \tValidation Loss: 0.000668\n",
      "Validation loss decreased (0.000728 --> 0.000668).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.586224 \tValidation Loss: 0.000945\n",
      "Epoch: 21 \tTraining Loss: 0.580880 \tValidation Loss: 0.001261\n",
      "Epoch: 22 \tTraining Loss: 0.576171 \tValidation Loss: 0.000772\n",
      "Epoch: 23 \tTraining Loss: 0.571229 \tValidation Loss: 0.000627\n",
      "Validation loss decreased (0.000668 --> 0.000627).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.566913 \tValidation Loss: 0.002431\n",
      "Epoch: 25 \tTraining Loss: 0.562784 \tValidation Loss: 0.002508\n",
      "Epoch: 26 \tTraining Loss: 0.558904 \tValidation Loss: 0.001034\n",
      "Epoch: 27 \tTraining Loss: 0.555221 \tValidation Loss: 0.000858\n",
      "Epoch: 28 \tTraining Loss: 0.551576 \tValidation Loss: 0.000779\n",
      "Epoch: 29 \tTraining Loss: 0.548198 \tValidation Loss: 0.000979\n",
      "Epoch: 30 \tTraining Loss: 0.545059 \tValidation Loss: 0.000773\n",
      "Epoch: 31 \tTraining Loss: 0.541752 \tValidation Loss: 0.000608\n",
      "Validation loss decreased (0.000627 --> 0.000608).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.538714 \tValidation Loss: 0.000322\n",
      "Validation loss decreased (0.000608 --> 0.000322).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.536149 \tValidation Loss: 0.000996\n",
      "Epoch: 34 \tTraining Loss: 0.533195 \tValidation Loss: 0.001302\n",
      "Epoch: 35 \tTraining Loss: 0.530439 \tValidation Loss: 0.001476\n",
      "Epoch: 36 \tTraining Loss: 0.528167 \tValidation Loss: 0.000738\n",
      "Epoch: 37 \tTraining Loss: 0.525274 \tValidation Loss: 0.001048\n",
      "Epoch: 38 \tTraining Loss: 0.523244 \tValidation Loss: 0.000746\n",
      "Epoch: 39 \tTraining Loss: 0.520969 \tValidation Loss: 0.001108\n",
      "Epoch: 40 \tTraining Loss: 0.518938 \tValidation Loss: 0.001035\n",
      "Epoch: 41 \tTraining Loss: 0.516857 \tValidation Loss: 0.000866\n",
      "Epoch: 42 \tTraining Loss: 0.514413 \tValidation Loss: 0.000931\n",
      "Epoch: 43 \tTraining Loss: 0.512849 \tValidation Loss: 0.000991\n",
      "Epoch: 44 \tTraining Loss: 0.510667 \tValidation Loss: 0.000994\n",
      "Epoch: 45 \tTraining Loss: 0.509085 \tValidation Loss: 0.000759\n",
      "Epoch: 46 \tTraining Loss: 0.507058 \tValidation Loss: 0.000885\n",
      "Epoch: 47 \tTraining Loss: 0.505442 \tValidation Loss: 0.000530\n",
      "Epoch: 48 \tTraining Loss: 0.503535 \tValidation Loss: 0.000954\n",
      "Epoch: 49 \tTraining Loss: 0.501365 \tValidation Loss: 0.000877\n",
      "Epoch: 50 \tTraining Loss: 0.500215 \tValidation Loss: 0.000881\n",
      "Epoch: 51 \tTraining Loss: 0.498585 \tValidation Loss: 0.000678\n",
      "Epoch: 52 \tTraining Loss: 0.497323 \tValidation Loss: 0.000536\n",
      "Epoch: 53 \tTraining Loss: 0.495496 \tValidation Loss: 0.001436\n",
      "Epoch: 54 \tTraining Loss: 0.494208 \tValidation Loss: 0.001264\n",
      "Epoch: 55 \tTraining Loss: 0.492627 \tValidation Loss: 0.002123\n",
      "Epoch: 56 \tTraining Loss: 0.491180 \tValidation Loss: 0.000529\n",
      "Epoch: 57 \tTraining Loss: 0.490055 \tValidation Loss: 0.000325\n",
      "Epoch: 58 \tTraining Loss: 0.488708 \tValidation Loss: 0.000799\n",
      "Epoch: 59 \tTraining Loss: 0.487529 \tValidation Loss: 0.000742\n",
      "Epoch: 60 \tTraining Loss: 0.485610 \tValidation Loss: 0.000727\n",
      "Epoch: 61 \tTraining Loss: 0.484550 \tValidation Loss: 0.001045\n",
      "Epoch: 62 \tTraining Loss: 0.483655 \tValidation Loss: 0.000388\n",
      "Epoch: 63 \tTraining Loss: 0.482205 \tValidation Loss: 0.000407\n",
      "Epoch: 64 \tTraining Loss: 0.481183 \tValidation Loss: 0.000648\n",
      "Epoch: 65 \tTraining Loss: 0.479939 \tValidation Loss: 0.000560\n",
      "Epoch: 66 \tTraining Loss: 0.479232 \tValidation Loss: 0.000829\n",
      "Epoch: 67 \tTraining Loss: 0.477919 \tValidation Loss: 0.000523\n",
      "Epoch: 68 \tTraining Loss: 0.476882 \tValidation Loss: 0.000449\n",
      "Epoch: 69 \tTraining Loss: 0.476128 \tValidation Loss: 0.000887\n",
      "Epoch: 70 \tTraining Loss: 0.474965 \tValidation Loss: 0.000851\n",
      "Epoch: 71 \tTraining Loss: 0.474023 \tValidation Loss: 0.000507\n",
      "Epoch: 72 \tTraining Loss: 0.473254 \tValidation Loss: 0.000611\n",
      "Epoch: 73 \tTraining Loss: 0.471927 \tValidation Loss: 0.000886\n",
      "Epoch: 74 \tTraining Loss: 0.471450 \tValidation Loss: 0.000622\n",
      "Epoch: 75 \tTraining Loss: 0.470038 \tValidation Loss: 0.000605\n",
      "Epoch: 76 \tTraining Loss: 0.469435 \tValidation Loss: 0.000442\n",
      "Epoch: 77 \tTraining Loss: 0.468730 \tValidation Loss: 0.000546\n",
      "Epoch: 78 \tTraining Loss: 0.467667 \tValidation Loss: 0.000898\n",
      "Epoch: 79 \tTraining Loss: 0.466836 \tValidation Loss: 0.000439\n",
      "Epoch: 80 \tTraining Loss: 0.466013 \tValidation Loss: 0.000560\n",
      "Epoch: 81 \tTraining Loss: 0.465357 \tValidation Loss: 0.001007\n",
      "Epoch: 82 \tTraining Loss: 0.464695 \tValidation Loss: 0.001053\n",
      "Epoch: 83 \tTraining Loss: 0.463593 \tValidation Loss: 0.000814\n",
      "Epoch: 84 \tTraining Loss: 0.462864 \tValidation Loss: 0.001257\n",
      "Epoch: 85 \tTraining Loss: 0.462085 \tValidation Loss: 0.000503\n",
      "Epoch: 86 \tTraining Loss: 0.461597 \tValidation Loss: 0.001297\n",
      "Epoch: 87 \tTraining Loss: 0.460770 \tValidation Loss: 0.000904\n",
      "Epoch: 88 \tTraining Loss: 0.459808 \tValidation Loss: 0.000881\n",
      "Epoch: 89 \tTraining Loss: 0.458988 \tValidation Loss: 0.000565\n",
      "Epoch: 90 \tTraining Loss: 0.459053 \tValidation Loss: 0.000871\n",
      "Epoch: 91 \tTraining Loss: 0.457887 \tValidation Loss: 0.000466\n",
      "Epoch: 92 \tTraining Loss: 0.457563 \tValidation Loss: 0.001189\n",
      "Epoch: 93 \tTraining Loss: 0.456470 \tValidation Loss: 0.000409\n",
      "Epoch: 94 \tTraining Loss: 0.455962 \tValidation Loss: 0.000633\n",
      "Epoch: 95 \tTraining Loss: 0.455383 \tValidation Loss: 0.000171\n",
      "Validation loss decreased (0.000322 --> 0.000171).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.454571 \tValidation Loss: 0.000866\n",
      "Epoch: 97 \tTraining Loss: 0.453866 \tValidation Loss: 0.001433\n",
      "Epoch: 98 \tTraining Loss: 0.453450 \tValidation Loss: 0.000543\n",
      "Epoch: 99 \tTraining Loss: 0.452869 \tValidation Loss: 0.000581\n",
      "Epoch: 100 \tTraining Loss: 0.452232 \tValidation Loss: 0.000417\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "     \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    newmodel.train() # prep model for training\n",
    "    for data,label in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = newmodel(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        prev = []\n",
    "        prev.append(copy.deepcopy(newmodel.fc1.weight))\n",
    "        prev.append(copy.deepcopy(newmodel.fc1.bias))\n",
    "        prev.append(copy.deepcopy(newmodel.fc2.weight))\n",
    "        prev.append(copy.deepcopy(newmodel.fc2.bias))\n",
    "        curr = []\n",
    "        curr.append(copy.deepcopy(newmodel.fc1.weight))\n",
    "        curr.append(copy.deepcopy(newmodel.fc1.bias))\n",
    "        curr.append(copy.deepcopy(newmodel.fc2.weight))\n",
    "        curr.append(copy.deepcopy(newmodel.fc2.bias))\n",
    "        \n",
    "        for i in range(4):\n",
    "            prev[i] = prev[i] * notmul[i]\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        curr[0] = curr[0]*mult[0]\n",
    "        curr[1] = curr[1]*mult[1]\n",
    "        curr[2] = curr[2]*mult[2]\n",
    "        curr[3] = curr[3]*mult[3]\n",
    "        \n",
    "        newmodel.fc1.weight = torch.nn.Parameter(curr[0] + prev[0])\n",
    "        newmodel.fc1.bias = torch.nn.Parameter(curr[1] + prev[1])\n",
    "        newmodel.fc2.weight = torch.nn.Parameter(curr[2] + prev[2])\n",
    "        newmodel.fc2.bias = torch.nn.Parameter(curr[3] + prev[3])\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        \n",
    "     ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    newmodel.eval()  # prep model for evaluation\n",
    "    for data,label in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = newmodel(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # update running validation loss \n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(newmodel.state_dict(), './fixnet/modelmean.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    if train_loss < 0.00000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8e5889d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.load_state_dict(torch.load('./fixnet/modelmean.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5dbcb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.489185\n",
      "\n",
      "Test Accuracy of     0: 95% (933/980)\n",
      "Test Accuracy of     1: 97% (1112/1135)\n",
      "Test Accuracy of     2: 87% (899/1032)\n",
      "Test Accuracy of     3: 82% (836/1010)\n",
      "Test Accuracy of     4: 72% (710/982)\n",
      "Test Accuracy of     5: 81% (731/892)\n",
      "Test Accuracy of     6: 90% (868/958)\n",
      "Test Accuracy of     7: 87% (898/1028)\n",
      "Test Accuracy of     8: 72% (702/974)\n",
      "Test Accuracy of     9: 76% (769/1009)\n",
      "\n",
      "Test Accuracy (Overall): 84% (8458/10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "newmodel.eval() # prep model for evaluation\n",
    "for data, target in test_loader:\n",
    "    output = newmodel(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2cc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# define NN architecture\n",
    "class FixNetMean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FixNetMean,self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 512\n",
    "        hidden_2 = 512\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(10, 512)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(512,10)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        # self.droput = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten image input\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        #x = self.droput(x)\n",
    "         # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        #x = self.droput(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2168328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.339501 \tValidation Loss: 0.000011\n",
      "Validation loss decreased (inf --> 0.000011).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.096339 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000011 --> 0.000000).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.090018 \tValidation Loss: 0.000003\n",
      "Epoch: 4 \tTraining Loss: 0.035976 \tValidation Loss: 0.000000\n",
      "Epoch: 5 \tTraining Loss: 0.028270 \tValidation Loss: 0.000000\n",
      "Epoch: 6 \tTraining Loss: 0.029549 \tValidation Loss: 0.000002\n",
      "Epoch: 7 \tTraining Loss: 0.026488 \tValidation Loss: 0.000000\n",
      "Epoch: 8 \tTraining Loss: 0.027351 \tValidation Loss: 0.000000\n",
      "Epoch: 9 \tTraining Loss: 0.021588 \tValidation Loss: 0.000000\n",
      "Epoch: 10 \tTraining Loss: 0.023199 \tValidation Loss: 0.000000\n",
      "Epoch: 11 \tTraining Loss: 0.022579 \tValidation Loss: 0.000000\n",
      "Epoch: 12 \tTraining Loss: 0.023297 \tValidation Loss: 0.001774\n",
      "Epoch: 13 \tTraining Loss: 0.019368 \tValidation Loss: 0.000000\n",
      "Epoch: 14 \tTraining Loss: 0.019253 \tValidation Loss: 0.000000\n",
      "Epoch: 15 \tTraining Loss: 0.020663 \tValidation Loss: 0.000001\n",
      "Epoch: 16 \tTraining Loss: 0.021482 \tValidation Loss: 0.000000\n",
      "Epoch: 17 \tTraining Loss: 0.022195 \tValidation Loss: 0.000001\n",
      "Epoch: 18 \tTraining Loss: 0.023939 \tValidation Loss: 0.000002\n",
      "Epoch: 19 \tTraining Loss: 0.018569 \tValidation Loss: 0.006405\n",
      "Epoch: 20 \tTraining Loss: 0.019214 \tValidation Loss: 0.000003\n",
      "Epoch: 21 \tTraining Loss: 0.017928 \tValidation Loss: 0.000000\n",
      "Epoch: 22 \tTraining Loss: 0.019808 \tValidation Loss: 0.000000\n",
      "Epoch: 23 \tTraining Loss: 0.019772 \tValidation Loss: 0.000164\n",
      "Epoch: 24 \tTraining Loss: 0.015649 \tValidation Loss: 0.000000\n",
      "Epoch: 25 \tTraining Loss: 0.017470 \tValidation Loss: 0.000001\n",
      "Epoch: 26 \tTraining Loss: 0.019216 \tValidation Loss: 0.000000\n",
      "Epoch: 27 \tTraining Loss: 0.023423 \tValidation Loss: 0.000000\n",
      "Epoch: 28 \tTraining Loss: 0.016772 \tValidation Loss: 0.000217\n",
      "Epoch: 29 \tTraining Loss: 0.021307 \tValidation Loss: 0.000000\n",
      "Epoch: 30 \tTraining Loss: 0.014064 \tValidation Loss: 0.000000\n",
      "Epoch: 31 \tTraining Loss: 0.020033 \tValidation Loss: 0.000000\n",
      "Epoch: 32 \tTraining Loss: 0.022461 \tValidation Loss: 0.000000\n",
      "Epoch: 33 \tTraining Loss: 0.020870 \tValidation Loss: 0.000004\n",
      "Epoch: 34 \tTraining Loss: 0.019127 \tValidation Loss: 0.000000\n",
      "Epoch: 35 \tTraining Loss: 0.023058 \tValidation Loss: 0.000000\n",
      "Epoch: 36 \tTraining Loss: 0.021251 \tValidation Loss: 0.000000\n",
      "Epoch: 37 \tTraining Loss: 0.017186 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.018513 \tValidation Loss: 0.000000\n",
      "Epoch: 39 \tTraining Loss: 0.018850 \tValidation Loss: 0.000921\n",
      "Epoch: 40 \tTraining Loss: 0.019904 \tValidation Loss: 0.000000\n",
      "Epoch: 41 \tTraining Loss: 0.019191 \tValidation Loss: 0.000995\n",
      "Epoch: 42 \tTraining Loss: 0.017183 \tValidation Loss: 0.000002\n",
      "Epoch: 43 \tTraining Loss: 0.019768 \tValidation Loss: 0.000000\n",
      "Epoch: 44 \tTraining Loss: 0.017542 \tValidation Loss: 0.000000\n",
      "Epoch: 45 \tTraining Loss: 0.030502 \tValidation Loss: 0.000000\n",
      "Epoch: 46 \tTraining Loss: 0.019425 \tValidation Loss: 0.000000\n",
      "Epoch: 47 \tTraining Loss: 0.021681 \tValidation Loss: 0.000000\n",
      "Epoch: 48 \tTraining Loss: 0.018538 \tValidation Loss: 0.000000\n",
      "Epoch: 49 \tTraining Loss: 0.020701 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.017782 \tValidation Loss: 0.000000\n",
      "Epoch: 51 \tTraining Loss: 0.022287 \tValidation Loss: 0.000000\n",
      "Epoch: 52 \tTraining Loss: 0.024370 \tValidation Loss: 0.000000\n",
      "Epoch: 53 \tTraining Loss: 0.018964 \tValidation Loss: 0.000000\n",
      "Epoch: 54 \tTraining Loss: 0.020012 \tValidation Loss: 0.000000\n",
      "Epoch: 55 \tTraining Loss: 0.021833 \tValidation Loss: 0.000000\n",
      "Epoch: 56 \tTraining Loss: 0.021978 \tValidation Loss: 0.000000\n",
      "Epoch: 57 \tTraining Loss: 0.030284 \tValidation Loss: 0.000000\n",
      "Epoch: 58 \tTraining Loss: 0.020448 \tValidation Loss: 0.000000\n",
      "Epoch: 59 \tTraining Loss: 0.023685 \tValidation Loss: 0.000000\n",
      "Epoch: 60 \tTraining Loss: 0.024051 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.028353 \tValidation Loss: 0.000000\n",
      "Epoch: 62 \tTraining Loss: 0.017436 \tValidation Loss: 0.000000\n",
      "Epoch: 63 \tTraining Loss: 0.022308 \tValidation Loss: 0.000008\n",
      "Epoch: 64 \tTraining Loss: 0.026600 \tValidation Loss: 0.000000\n",
      "Epoch: 65 \tTraining Loss: 0.025617 \tValidation Loss: 0.000000\n",
      "Epoch: 66 \tTraining Loss: 0.023916 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.021122 \tValidation Loss: 0.000001\n",
      "Epoch: 68 \tTraining Loss: 0.025063 \tValidation Loss: 0.000000\n",
      "Epoch: 69 \tTraining Loss: 0.025133 \tValidation Loss: 0.000000\n",
      "Epoch: 70 \tTraining Loss: 0.026227 \tValidation Loss: 0.000000\n",
      "Epoch: 71 \tTraining Loss: 0.027277 \tValidation Loss: 0.000000\n",
      "Epoch: 72 \tTraining Loss: 0.016475 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.025450 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.024274 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.022890 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.021912 \tValidation Loss: 0.000000\n",
      "Epoch: 77 \tTraining Loss: 0.025622 \tValidation Loss: 0.000000\n",
      "Epoch: 78 \tTraining Loss: 0.028059 \tValidation Loss: 0.000002\n",
      "Epoch: 79 \tTraining Loss: 0.022116 \tValidation Loss: 0.000000\n",
      "Epoch: 80 \tTraining Loss: 0.030607 \tValidation Loss: 0.000000\n",
      "Epoch: 81 \tTraining Loss: 0.019434 \tValidation Loss: 0.000000\n",
      "Epoch: 82 \tTraining Loss: 0.031532 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.020274 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.025122 \tValidation Loss: 0.000201\n",
      "Epoch: 85 \tTraining Loss: 0.027325 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.023350 \tValidation Loss: 0.000000\n",
      "Epoch: 87 \tTraining Loss: 0.032430 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.036725 \tValidation Loss: 0.000000\n",
      "Epoch: 89 \tTraining Loss: 0.021075 \tValidation Loss: 0.000001\n",
      "Epoch: 90 \tTraining Loss: 0.022642 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.033202 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.034644 \tValidation Loss: 0.000000\n",
      "Epoch: 93 \tTraining Loss: 0.024983 \tValidation Loss: 0.000000\n",
      "Epoch: 94 \tTraining Loss: 0.018750 \tValidation Loss: 0.000000\n",
      "Epoch: 95 \tTraining Loss: 0.028889 \tValidation Loss: 0.000008\n",
      "Epoch: 96 \tTraining Loss: 0.023799 \tValidation Loss: 0.000000\n",
      "Epoch: 97 \tTraining Loss: 0.023411 \tValidation Loss: 0.000000\n",
      "Epoch: 98 \tTraining Loss: 0.033438 \tValidation Loss: 0.000000\n",
      "Epoch: 99 \tTraining Loss: 0.029598 \tValidation Loss: 0.000000\n",
      "Epoch: 100 \tTraining Loss: 0.023416 \tValidation Loss: 0.000000\n",
      "Validation loss decreased (0.000000 --> 0.000000).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "newmodel = FixNetMean()\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(newmodel.parameters(),lr = 0.001)\n",
    "n_epochs = 100\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "     \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    newmodel.train() # prep model for training\n",
    "    for data,label in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        input = []\n",
    "        for i,model in enumerate(models):            \n",
    "            input.append(model(data))\n",
    "        input = torch.cat(input,dim=1)\n",
    "        output = newmodel(input)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        \n",
    "     ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    newmodel.eval()  # prep model for evaluation\n",
    "    for data,label in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        input = []\n",
    "        for i,model in enumerate(models):            \n",
    "            input.append(model(data))\n",
    "        input = torch.cat(input,dim=1)\n",
    "        output = newmodel(input)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # update running validation loss \n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler)\n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(newmodel.state_dict(), './fixnet/modelmean.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    if train_loss < 0.00000001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f0daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel = FixNetMean()\n",
    "newmodel.load_state_dict(torch.load(\"./fixnet/modelmean.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e64ecdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.664680\n",
      "\n",
      "Test Accuracy of     0: 98% (965/980)\n",
      "Test Accuracy of     1: 98% (1123/1135)\n",
      "Test Accuracy of     2: 97% (1002/1032)\n",
      "Test Accuracy of     3: 99% (1002/1010)\n",
      "Test Accuracy of     4: 98% (967/982)\n",
      "Test Accuracy of     5: 95% (856/892)\n",
      "Test Accuracy of     6: 96% (920/958)\n",
      "Test Accuracy of     7: 96% (988/1028)\n",
      "Test Accuracy of     8: 95% (933/974)\n",
      "Test Accuracy of     9: 96% (972/1009)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9728/10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "newmodel.eval() # prep model for evaluation\n",
    "for data, target in test_loader:\n",
    "    input = []\n",
    "    for i,model in enumerate(models):            \n",
    "        input.append(model(data))\n",
    "    input = torch.cat(input,dim=1)\n",
    "    output = newmodel(input)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
